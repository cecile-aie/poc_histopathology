{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df488b3-5502-4e48-b1a7-a9b980d7fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Vidage RAM GPU\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd76ce-cfaa-4218-b10f-f5aa7ec0e270",
   "metadata": {},
   "source": [
    "## üß± CELL 01 ‚Äî Imports & config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8147c759-4267-4c8c-95a7-2dd1a0f29d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time, math, json, numpy as np, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# --- Environnement dans le container ---\n",
    "PROJECT_ROOT = Path(\"/workspace\").resolve()  # ou adapte si diff√©rent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b8c427-17b0-4a9b-9844-31a8b10c260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 05_StyleGAN ‚Äî PixCell-PathoDuet integration (Lite) ===\n",
    "\n",
    "from p9dg.histo_dataset import HistoDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SAMPLES_DIR = (PROJECT_ROOT / \"samples\").resolve();    SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR    = (PROJECT_ROOT / \"checkpoints\").resolve();CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTI_DIR    = (PROJECT_ROOT / \"artifacts_pixcell\").resolve(); ARTI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Si d√©j√† d√©finis ailleurs, garde tes valeurs :\n",
    "Z_DIM      = globals().get(\"Z_DIM\", 512)\n",
    "BATCH_SIZE = globals().get(\"BATCH_SIZE\", 8)\n",
    "USE_AMP    = globals().get(\"USE_AMP\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b9846-9e94-40cd-ba75-5b1ff3e2cd04",
   "metadata": {},
   "source": [
    "## üß± CELL 2 ‚Äî PathoDuet: import robuste + instance + freeze + encode_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95c7be3-e1a4-4bd3-a630-43291ce04050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PathoDuet] missing: []  | unexpected: ['pretext_token']\n",
      "[PathoDuet] backend=local_deit_b16_ckptHE  | EMB_DIM=768  | device=cuda\n"
     ]
    }
   ],
   "source": [
    "# === CELL 2 ‚Äî PathoDuet: import robuste + instance + freeze + encode_image (adapt√© ckpt HE) ===\n",
    "import re, os, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) Si tu as DEJA un objet ou une classe dans le notebook, on le r√©utilise tel quel\n",
    "if 'pathoduet' in globals():\n",
    "    _pd_backend = 'existing_object'\n",
    "    _pd_model = pathoduet\n",
    "elif 'PathoDuetModel' in globals():\n",
    "    _pd_backend = 'existing_class'\n",
    "    _pd_model = PathoDuetModel.from_pretrained(\"pathoduet-base\")\n",
    "else:\n",
    "    # 2) Chargement LOCAL conforme √† ton snippet (timm DeiT-B/16 + checkpoint_HE)\n",
    "    _pd_backend = 'local_deit_b16_ckptHE'\n",
    "    try:\n",
    "        import timm\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Installe 'timm' (pip install timm)\") from e\n",
    "\n",
    "    assert 'PROJECT_ROOT' in globals(), \"PROJECT_ROOT non d√©fini.\"\n",
    "    CKPT_PATH = PROJECT_ROOT / \"models/checkpoint_HE.pth\"\n",
    "    assert CKPT_PATH.exists(), f\"Checkpoint introuvable: {CKPT_PATH}\"\n",
    "\n",
    "    def load_state_dict_safely(path):\n",
    "        sd_raw = None\n",
    "        try:\n",
    "            obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "            if isinstance(obj, dict) and all(hasattr(v, \"shape\") for v in obj.values()):\n",
    "                sd_raw = obj\n",
    "            elif isinstance(obj, dict):\n",
    "                for k in (\"state_dict\",\"model\",\"module\"):\n",
    "                    if k in obj and isinstance(obj[k], dict) and all(hasattr(v, \"shape\") for v in obj[k].values()):\n",
    "                        sd_raw = obj[k]; break\n",
    "        except TypeError:\n",
    "            obj = torch.load(path, map_location=\"cpu\")\n",
    "            if isinstance(obj, dict) and all(hasattr(v, \"shape\") for v in obj.values()):\n",
    "                sd_raw = obj\n",
    "            elif isinstance(obj, dict):\n",
    "                for k in (\"state_dict\",\"model\",\"module\"):\n",
    "                    if k in obj and isinstance(obj[k], dict) and all(hasattr(v, \"shape\") for v in obj[k].values()):\n",
    "                        sd_raw = obj[k]; break\n",
    "        if sd_raw is None:\n",
    "            raise RuntimeError(\"state_dict non trouv√© dans le ckpt\")\n",
    "\n",
    "        clean_sd = {re.sub(r\"^(module\\.|model\\.)\",\"\",k): v for k, v in sd_raw.items()}\n",
    "        # retirer distillation + r√©ajuster pos_embed 198‚Üí197\n",
    "        clean_sd.pop(\"dist_token\", None)\n",
    "        for k in list(clean_sd.keys()):\n",
    "            if k.startswith(\"head_dist.\"): clean_sd.pop(k)\n",
    "        if \"pos_embed\" in clean_sd and clean_sd[\"pos_embed\"].shape[1] == 198:\n",
    "            pos = clean_sd[\"pos_embed\"]\n",
    "            clean_sd[\"pos_embed\"] = torch.cat([pos[:, :1, :], pos[:, 2:, :]], dim=1)\n",
    "        for k in list(clean_sd.keys()):\n",
    "            if k.startswith(\"head.\"): clean_sd.pop(k)\n",
    "        return clean_sd\n",
    "\n",
    "    state_dict = load_state_dict_safely(CKPT_PATH)\n",
    "    backbone = timm.create_model(\n",
    "        \"deit_base_patch16_224\",\n",
    "        pretrained=False,\n",
    "        num_classes=0,    # vecteur feature directement\n",
    "        img_size=224,\n",
    "        global_pool=\"avg\" # GAP ‚Üí (B, 768)\n",
    "    )\n",
    "    missing, unexpected = backbone.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[PathoDuet] missing: {missing[:5]}  | unexpected: {unexpected[:5]}\")\n",
    "\n",
    "    for p in backbone.parameters(): p.requires_grad_(False)\n",
    "    backbone.eval()\n",
    "\n",
    "    class _PathoDuetExtractor(nn.Module):\n",
    "        \"\"\"\n",
    "        Attend x dans [-1,1]. Redimensionne en 224x224. Retourne embedding (B, D).\n",
    "        \"\"\"\n",
    "        def __init__(self, bb, in_size=224):\n",
    "            super().__init__()\n",
    "            self.bb = bb\n",
    "            self.in_size = in_size\n",
    "            self.dim = None\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def forward(self, x_m11: torch.Tensor) -> torch.Tensor:\n",
    "            # [-1,1] -> [0,1]\n",
    "            x01 = (x_m11.clamp(-1,1) + 1)*0.5\n",
    "            if x01.shape[-2:] != (self.in_size, self.in_size):\n",
    "                x01 = F.interpolate(x01, size=(self.in_size,self.in_size),\n",
    "                                    mode=\"bilinear\", align_corners=False)\n",
    "            feats = self.bb(x01)               # (B, D) attendu (D=768 sur DeiT-B/16)\n",
    "            if self.dim is None: self.dim = feats.shape[-1]\n",
    "            return feats\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def encode_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            return self.forward(x)\n",
    "\n",
    "    _pd_model = _PathoDuetExtractor(backbone)\n",
    "\n",
    "# 3) Normaliser l‚ÄôAPI: garantir pathoduet.encode_image(x) -> (B, EMB_DIM) L2-normalis√©\n",
    "class _EncodeImageWrapper(nn.Module):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.dim = None  # sera inf√©r√© au 1er appel\n",
    "        self.base.eval()\n",
    "        for p in self.base.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        self._has_encode = hasattr(self.base, \"encode_image\") and callable(getattr(self.base, \"encode_image\"))\n",
    "        self._has_forward = hasattr(self.base, \"forward\") and callable(getattr(self.base, \"forward\"))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode_image(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,3,H,W) float dans [-1,1] (conforme √† ton pipeline GAN), ou [0,1] si d√©j√† remapp√©.\n",
    "        return: (B, EMB_DIM) L2-normalis√©\n",
    "        \"\"\"\n",
    "        self.base.eval()\n",
    "        if self._has_encode:\n",
    "            feats = self.base.encode_image(x)\n",
    "        else:\n",
    "            out_raw = self.base(x) if self._has_forward else self.base.forward(x)\n",
    "            if isinstance(out_raw, torch.Tensor):\n",
    "                feats = out_raw\n",
    "            elif hasattr(out_raw, \"last_hidden_state\"):\n",
    "                feats = out_raw.last_hidden_state.mean(dim=1)\n",
    "            elif hasattr(out_raw, \"pooler_output\"):\n",
    "                feats = out_raw.pooler_output\n",
    "            else:\n",
    "                tensors = [v for v in out_raw.__dict__.values() if isinstance(v, torch.Tensor)]\n",
    "                if not tensors:\n",
    "                    raise RuntimeError(\"Impossible d'extraire des features du mod√®le PathoDuet (structure inconnue).\")\n",
    "                feats = tensors[0]\n",
    "            if feats.ndim == 3:   # (B, T, D)\n",
    "                feats = feats.mean(dim=1)\n",
    "            elif feats.ndim > 2:  # (B, C, H, W) -> GAP\n",
    "                feats = F.adaptive_avg_pool2d(feats, 1).flatten(1)\n",
    "\n",
    "        if self.dim is None:\n",
    "            self.dim = feats.shape[-1]\n",
    "        return F.normalize(feats, dim=-1)\n",
    "\n",
    "# 4) Exposer l‚Äôinstance normalis√©e `pathoduet` et EMB_DIM (on force une passe dummy pour fixer dim)\n",
    "pathoduet = _EncodeImageWrapper(_pd_model).to(DEVICE).eval()\n",
    "with torch.no_grad():\n",
    "    _dummy = torch.zeros(1,3,256,256, device=DEVICE)  # taille quelconque\n",
    "    _ = pathoduet.encode_image(_dummy)\n",
    "EMB_DIM = pathoduet.dim or 768\n",
    "\n",
    "print(f\"[PathoDuet] backend={_pd_backend}  | EMB_DIM={EMB_DIM}  | device={DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90242ac-e63f-46ee-ae03-8641d551016a",
   "metadata": {},
   "source": [
    "## üß± CELL 3 ‚Äî Cache d'embeddings PathoDuet (train/val) + prototypes Œº_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a42a69-0287-498d-81d8-8d81d3c7cadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® R√©f√©rence Vahadane fix√©e : TUM-RQEVGAED.tif\n",
      "üé® R√©f√©rence Vahadane auto: TUM-RQEVGAED.tif\n",
      "‚úÖ Seuils par classe charg√©s depuis : /workspace/configs/seuils_par_classe.json\n",
      "‚öñÔ∏è √âchantillonnage √©quilibr√© auto (8763 images / classe, min du dataset).\n",
      "[ds_gan] 78867 items | classes: {'ADI': 10407, 'BACK': 10566, 'DEB': 11512, 'LYM': 11557, 'MUC': 8896, 'MUS': 13536, 'NORM': 8763, 'STR': 10446, 'TUM': 14317}\n",
      "üé® R√©f√©rence Vahadane fix√©e : TUM-RQEVGAED.tif\n",
      "üé® R√©f√©rence Vahadane auto: TUM-RQEVGAED.tif\n",
      "‚úÖ Seuils par classe charg√©s depuis : /workspace/configs/seuils_par_classe.json\n",
      "‚öñÔ∏è √âchantillonnage √©quilibr√© auto (8763 images / classe, min du dataset).\n",
      "[embeddings/train] classes (9): {'ADI': 0, 'BACK': 1, 'DEB': 2, 'LYM': 3, 'MUC': 4, 'MUS': 5, 'NORM': 6, 'STR': 7, 'TUM': 8}\n",
      "[cache] train: charg√©s (4500, 768) depuis train_feats_np500.npy\n",
      "Œº_k pr√™ts ‚Üí {'ADI': (768,), 'BACK': (768,), 'DEB': (768,)}\n"
     ]
    }
   ],
   "source": [
    "# === CELL 3 bis ‚Äî Datasets (GAN + Embeddings PathoDuet) + √âCHANTILLONNAGE + cache + Œº_k (train only) ===\n",
    "import os, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from p9dg.histo_dataset import HistoDataset\n",
    "\n",
    "# ---- Chemins & config (conteneur) ----\n",
    "DATA_ROOT   = Path(os.getenv(\"DATA_ROOT\", \"/workspace/data\")).resolve()\n",
    "CONFIG_DIR  = Path(os.getenv(\"CONFIG_DIR\", str(PROJECT_ROOT))).resolve()\n",
    "THR_PATH    = CONFIG_DIR / \"seuils_par_classe.json\"\n",
    "assert THR_PATH.exists(), f\"Fichier de seuils introuvable: {THR_PATH}\"\n",
    "\n",
    "# ---- R√©glages (reprend ceux d√©finis plus haut si pr√©sents) ----\n",
    "IMAGE_SIZE        = globals().get(\"IMAGE_SIZE\", 128)\n",
    "PIXEL_RANGE       = globals().get(\"PIXEL_RANGE\", \"-1_1\")\n",
    "VAHADANE_ENABLE   = globals().get(\"VAHADANE_ENABLE\", True)\n",
    "SAMPLES_PER_CLASS = globals().get(\"SAMPLES_PER_CLASS\", None)\n",
    "SELECTED_CLASSES  = globals().get(\"SELECTED_CLASSES\", None)\n",
    "NUM_WORKERS       = globals().get(\"NUM_WORKERS\", 0)\n",
    "DEVICE            = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---- Contr√¥le de l'effort d'extraction embeddings ----\n",
    "EMB_MAX_PER_CLASS = globals().get(\"EMB_MAX_PER_CLASS\", 500)  # ‚Üê budget par classe (ex: 500)\n",
    "BATCH_EMB         = globals().get(\"BATCH_EMB\", 128)\n",
    "EXTRACT_VAL_EMB   = globals().get(\"EXTRACT_VAL_EMB\", False)  # ‚Üê par d√©faut, on N'EXTRAIT PAS sur val\n",
    "\n",
    "# ---- Dossiers artefacts ----\n",
    "ARTI_DIR = (PROJECT_ROOT / \"artifacts_pixcell\").resolve(); ARTI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMB_DIR  = (ARTI_DIR / \"emb_cache\").resolve(); EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "#  A) Dataset principal GAN (inchang√©)\n",
    "# ============================\n",
    "ds_gan = HistoDataset(\n",
    "    root_data=str(DATA_ROOT),\n",
    "    split=\"train\",\n",
    "    output_size=IMAGE_SIZE,\n",
    "    pixel_range=PIXEL_RANGE,\n",
    "    balance_per_class=True,\n",
    "    samples_per_class_per_epoch=SAMPLES_PER_CLASS,\n",
    "    vahadane_enable=VAHADANE_ENABLE,\n",
    "    vahadane_device=DEVICE,\n",
    "    thresholds_json_path=str(THR_PATH),\n",
    "    return_labels=True,\n",
    "    classes=SELECTED_CLASSES,\n",
    ")\n",
    "print(f\"[ds_gan] {len(ds_gan)} items | classes: {ds_gan.class_counts()}\")\n",
    "\n",
    "# ============================\n",
    "#  B) Dataset embeddings (TRAIN uniquement, en -1_1)\n",
    "# ============================\n",
    "ds_tr_emb = HistoDuet = HistoDataset(\n",
    "    root_data=str(DATA_ROOT),\n",
    "    split=\"train\",\n",
    "    output_size=IMAGE_SIZE,\n",
    "    pixel_range=\"-1_1\",             # PathoDuet attend [-1,1]\n",
    "    balance_per_class=True,         # donne un flux √©quilibr√©\n",
    "    samples_per_class_per_epoch=None,  # on ma√Ætrise l'√©chantillonnage nous-m√™mes ci-dessous\n",
    "    vahadane_enable=VAHADANE_ENABLE,\n",
    "    vahadane_device=DEVICE,\n",
    "    thresholds_json_path=str(THR_PATH),\n",
    "    return_labels=True,\n",
    "    classes=SELECTED_CLASSES,\n",
    ")\n",
    "dl_tr_emb = DataLoader(ds_tr_emb, batch_size=BATCH_EMB, shuffle=True, num_workers=NUM_WORKERS, pin_memory=False)\n",
    "class_to_idx = ds_tr_emb.class_to_idx\n",
    "idx_to_class = ds_tr_emb.idx_to_class\n",
    "num_classes  = len(class_to_idx)\n",
    "print(f\"[embeddings/train] classes ({num_classes}): {class_to_idx}\")\n",
    "\n",
    "# ============================\n",
    "#  C) Extraction + cache (avec PLAFOND par classe)\n",
    "# ============================\n",
    "assert 'pathoduet' in globals() and 'EMB_DIM' in globals(), \"Ex√©cute d'abord la cellule PathoDuet (Cell 2).\"\n",
    "\n",
    "def cache_embeddings_capped(dl, split_name: str, max_per_class: int):\n",
    "    fpath = EMB_DIR / f\"{split_name}_feats_np{max_per_class}.npy\"\n",
    "    lpath = EMB_DIR / f\"{split_name}_labels_np{max_per_class}.npy\"\n",
    "    if fpath.exists() and lpath.exists():\n",
    "        feats = torch.from_numpy(np.load(fpath))\n",
    "        labs  = torch.from_numpy(np.load(lpath))\n",
    "        print(f\"[cache] {split_name}: charg√©s {tuple(feats.shape)} depuis {fpath.name}\")\n",
    "        return feats, labs\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    all_f, all_y = [], []\n",
    "    total_target = max_per_class * num_classes\n",
    "\n",
    "    pathoduet.eval()\n",
    "    with torch.no_grad():\n",
    "        for xb, yb, _ in dl:\n",
    "            # stop si on a atteint le budget global\n",
    "            if sum(counts.values()) >= total_target:\n",
    "                break\n",
    "\n",
    "            xb = xb.to(DEVICE, non_blocking=True)\n",
    "            fb = pathoduet.encode_image(xb)  # (B, EMB_DIM), L2-normalis√©\n",
    "\n",
    "            # filtrage par classe pour respecter max_per_class\n",
    "            for f_row, y in zip(fb.cpu(), yb.cpu()):\n",
    "                if counts[int(y)] < max_per_class:\n",
    "                    all_f.append(f_row.unsqueeze(0))\n",
    "                    all_y.append(torch.tensor([int(y)]))\n",
    "                    counts[int(y)] += 1\n",
    "                # early stop si on a rempli toutes les classes\n",
    "                if len(counts) == num_classes and min(counts.values()) >= max_per_class:\n",
    "                    break\n",
    "\n",
    "    if not all_f:\n",
    "        raise RuntimeError(\"Aucun embedding collect√© ‚Äî v√©rifie les chemins et le budget EMB_MAX_PER_CLASS.\")\n",
    "    F = torch.cat(all_f, 0)  # (K, EMB_DIM)\n",
    "    Y = torch.cat(all_y, 0).view(-1)  # (K,)\n",
    "    np.save(fpath, F.numpy()); np.save(lpath, Y.numpy())\n",
    "    print(f\"[cache] {split_name}: sauvegard√©s {tuple(F.shape)} ‚Üí {fpath.name}\")\n",
    "    print(f\"[budget par classe] \", {idx_to_class[k]: c for k,c in counts.items()})\n",
    "    return F, Y\n",
    "\n",
    "Ftr, Ytr = cache_embeddings_capped(dl_tr_emb, \"train\", EMB_MAX_PER_CLASS)\n",
    "\n",
    "# (Optionnel) banque embeddings val ‚Äî par d√©faut d√©sactiv√©e pour √©viter tout m√©lange\n",
    "if EXTRACT_VAL_EMB:\n",
    "    ds_va_emb = HistoDataset(\n",
    "        root_data=str(DATA_ROOT),\n",
    "        split=\"val\",\n",
    "        output_size=IMAGE_SIZE,\n",
    "        pixel_range=\"-1_1\",\n",
    "        balance_per_class=True,\n",
    "        samples_per_class_per_epoch=None,\n",
    "        vahadane_enable=VAHADANE_ENABLE,\n",
    "        vahadane_device=DEVICE,\n",
    "        thresholds_json_path=str(THR_PATH),\n",
    "        return_labels=True,\n",
    "        classes=SELECTED_CLASSES,\n",
    "    )\n",
    "    dl_va_emb = DataLoader(ds_va_emb, batch_size=BATCH_EMB, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    Fva, Yva = cache_embeddings_capped(dl_va_emb, \"val\", EMB_MAX_PER_CLASS)\n",
    "else:\n",
    "    Fva, Yva = None, None\n",
    "\n",
    "# ============================\n",
    "#  D) Prototypes Œº_k (sur train)\n",
    "# ============================\n",
    "@torch.no_grad()\n",
    "def class_prototypes(F: torch.Tensor, Y: torch.Tensor):\n",
    "    protos = {}\n",
    "    for k in torch.unique(Y):\n",
    "        m = (Y == int(k))\n",
    "        protos[int(k)] = F[m].mean(0)   # (EMB_DIM,)\n",
    "    return protos\n",
    "\n",
    "PROTOS = class_prototypes(Ftr, Ytr)\n",
    "print(\"Œº_k pr√™ts ‚Üí\", {idx_to_class[k]: tuple(v.shape) for k, v in list(PROTOS.items())[:min(3, len(PROTOS))]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ab9a3-1898-430c-9a38-808251303a10",
   "metadata": {},
   "source": [
    "## üß± CELL 4 ‚Äî Adapter (EMB_DIM ‚Üí C_DIM) + utilitaires Œº_k ‚Üí c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8b961a-e431-4b1a-a099-111bda568639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Adapter] C_DIM=128 | c_test shape=(3, 128)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "assert 'EMB_DIM' in globals() and 'PROTOS' in globals(), \"Ex√©cute d'abord Cell 2 (PathoDuet) et Cell 3 (Œº_k).\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Dimension de condition c√¥t√© GAN (modifiable)\n",
    "C_DIM = globals().get(\"C_DIM\", 128)\n",
    "\n",
    "class PixCellAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Projette l'embedding PathoDuet (EMB_DIM) vers l'espace de condition (C_DIM).\n",
    "    Petit MLP suffisant pour le POC.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int = EMB_DIM, out_dim: int = C_DIM):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256), nn.SiLU(),\n",
    "            nn.Linear(256, out_dim)\n",
    "        )\n",
    "    def forward(self, e: torch.Tensor) -> torch.Tensor:\n",
    "        # e: (B, EMB_DIM) ‚Äî d√©j√† L2-normalis√© par PathoDuet\n",
    "        return self.net(e)\n",
    "\n",
    "adapter = PixCellAdapter().to(DEVICE).eval()  # on l'entra√Ænera plus tard (Cellule entra√Ænement \"lite\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def proto_tensor(k: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Retourne le prototype Œº_k (EMB_DIM,) sur le bon device.\n",
    "    \"\"\"\n",
    "    e = PROTOS[int(k)].to(DEVICE)\n",
    "    return e\n",
    "\n",
    "@torch.no_grad()\n",
    "def cond_from_proto(labels: torch.Tensor, noise_std: float = 0.0, mixup_alpha: float | None = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convertit une liste d'indices de classe ‚Üí vecteurs de condition c via:\n",
    "      1) r√©cup√©ration des prototypes Œº_k\n",
    "      2) option: bruit gaussien dans l'espace embedding\n",
    "      3) projection via l'Adapter (EMB_DIM ‚Üí C_DIM)\n",
    "\n",
    "    Args:\n",
    "        labels: (B,) tensor long\n",
    "        noise_std: std du bruit dans l'espace PathoDuet (petite diversit√© intra-classe)\n",
    "        mixup_alpha: si d√©fini, m√©lange de deux prototypes (Beta(alpha, alpha)) pour diversifier\n",
    "\n",
    "    Returns:\n",
    "        c: (B, C_DIM)\n",
    "    \"\"\"\n",
    "    B = labels.numel()\n",
    "    E = []\n",
    "    for k in labels.tolist():\n",
    "        e = proto_tensor(k)\n",
    "        if mixup_alpha is not None:\n",
    "            # mixup entre Œº_k et un autre prototype al√©atoire\n",
    "            k2 = int(torch.randint(low=0, high=len(PROTOS), size=(1,)).item())\n",
    "            e2 = proto_tensor(k2)\n",
    "            lam = torch.distributions.Beta(mixup_alpha, mixup_alpha).sample().to(DEVICE)\n",
    "            e = lam * e + (1 - lam) * e2\n",
    "            e = F.normalize(e, dim=0)\n",
    "        if noise_std > 0:\n",
    "            e = e + noise_std * torch.randn_like(e)\n",
    "            e = F.normalize(e, dim=0)\n",
    "        E.append(e)\n",
    "    E = torch.stack(E, 0)       # (B, EMB_DIM)\n",
    "    c = adapter(E)              # (B, C_DIM)\n",
    "    return c\n",
    "\n",
    "# --- Sanity check rapide ---\n",
    "with torch.no_grad():\n",
    "    dummy_labels = torch.tensor([0, 1, 2], device=DEVICE)  # adapte si ton mapping d√©bute ailleurs\n",
    "    c_test = cond_from_proto(dummy_labels, noise_std=0.0)\n",
    "print(f\"[Adapter] C_DIM={C_DIM} | c_test shape={tuple(c_test.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6f4e-332f-49fb-8360-ca5232ec40e4",
   "metadata": {},
   "source": [
    "## üß± CELL 5 ‚Äî Wrappers conditionnels pour G et D (z' = z + M(c), projection trick c√¥t√© D) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a86b68e-a97a-4c95-8e07-09a8ede4c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G params: 5,775,747 | D params: 6,959,553\n"
     ]
    }
   ],
   "source": [
    "# === CELL GD-1 ‚Äî G√©n√©rateur & Discriminateur jouets (compatibles [-1,1], logits (B,)) ===\n",
    "import math, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Valeurs par d√©faut si non d√©finies\n",
    "IMAGE_SIZE = globals().get(\"IMAGE_SIZE\", 256)\n",
    "Z_DIM      = globals().get(\"Z_DIM\", 512)\n",
    "\n",
    "assert IMAGE_SIZE in (64, 128, 256), \"Ce G/D jouet supporte 64, 128 ou 256 pour simplifier.\"\n",
    "\n",
    "# --------- Utils ---------\n",
    "def count_params(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "def weights_init_leaky(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, a=0.2, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "# --------- Blocks ---------\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(8, out_ch)\n",
    "        self.act  = nn.SiLU()\n",
    "    def forward(self, x):\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        x = self.conv(x)\n",
    "        x = self.norm(x)\n",
    "        return self.act(x)\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 4, stride=2, padding=1)\n",
    "        self.act  = nn.LeakyReLU(0.2, inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "# --------- G√©n√©rateur ---------\n",
    "class ToyGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    z -> linear -> (C0, 4, 4) -> upsample xN -> conv -> Tanh\n",
    "    Sortie en [-1,1].\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim=Z_DIM, img_size=IMAGE_SIZE, base=512):\n",
    "        super().__init__()\n",
    "        self.z_dim   = z_dim\n",
    "        self.img_sz  = img_size\n",
    "        self.map     = nn.Linear(z_dim, base*4*4)\n",
    "\n",
    "        # √âchelle de canaux selon image size\n",
    "        # 4 -> 8 -> 16 -> 32 -> 64 -> 128 -> 256\n",
    "        chs = [base, 256, 128, 64, 32, 16, 8]\n",
    "        depth_needed = {64: 4, 128: 5, 256: 6}[img_size]  # nb d'upsamples √† partir de 4x4\n",
    "        blocks = []\n",
    "        in_c = chs[0]\n",
    "        for i in range(depth_needed):\n",
    "            out_c = chs[i+1]\n",
    "            blocks.append(UpBlock(in_c, out_c))\n",
    "            in_c = out_c\n",
    "        self.ups = nn.Sequential(*blocks)\n",
    "        self.to_rgb = nn.Conv2d(in_c, 3, 3, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        b = z.size(0)\n",
    "        x = self.map(z).view(b, -1, 4, 4)\n",
    "        x = self.ups(x)\n",
    "        x = torch.tanh(self.to_rgb(x))   # [-1, 1]\n",
    "        return x\n",
    "\n",
    "# --------- Discriminateur ---------\n",
    "class ToyDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    PatchGAN-like global : downsample jusqu'√† 4x4, puis FC -> 1\n",
    "    Sortie: logit (B,)\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=IMAGE_SIZE, base=64):\n",
    "        super().__init__()\n",
    "        # 3 -> 64 -> 128 -> 256 -> 512 -> 512 -> ...\n",
    "        chans = [3, base, base*2, base*4, base*8, base*8]\n",
    "        blocks = []\n",
    "        for i in range(len(chans)-1):\n",
    "            blocks.append(DownBlock(chans[i], chans[i+1]))\n",
    "        self.down = nn.Sequential(*blocks)\n",
    "\n",
    "        # Taille spatiale apr√®s downsamples 2^n\n",
    "        # On applique n downsamples pour atteindre 4x4\n",
    "        n_down = int(math.log2(img_size//4))\n",
    "        # La s√©quence ci-dessus fait len(chans)-1 downsamples ; c'est suffisant pour 256/128/64\n",
    "        # On adapte le pooling final au spatial restant\n",
    "        self.pool = nn.AdaptiveAvgPool2d((4,4))\n",
    "        self.fc   = nn.Linear(chans[-1]*4*4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.down(x)\n",
    "        h = self.pool(h)\n",
    "        h = h.flatten(1)\n",
    "        logit = self.fc(h).squeeze(1)  # (B,)\n",
    "        return logit\n",
    "\n",
    "# --------- Instanciation + init ---------\n",
    "G = ToyGenerator(z_dim=Z_DIM, img_size=IMAGE_SIZE).to(DEVICE)\n",
    "D = ToyDiscriminator(img_size=IMAGE_SIZE).to(DEVICE)\n",
    "G.apply(weights_init_leaky)\n",
    "D.apply(weights_init_leaky)\n",
    "\n",
    "print(f\"G params: {count_params(G):,} | D params: {count_params(D):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9fcbb7-1d4e-478f-bfe2-8af99364f913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Wrappers] OK ‚Üí Gc(z,c)‚Üí(4, 3, 128, 128) ; Dc(x,c)‚Üí(B,)\n",
      "[Freeze] G & D gel√©s ; t√™tes conditionnelles (Gc.map, Dc.hproj, Dc.proj) entra√Ænables.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "assert 'G' in globals() and 'D' in globals(), \"Instancie d'abord tes mod√®les G et D.\"\n",
    "assert 'Z_DIM' in globals() and 'C_DIM' in globals(), \"Z_DIM et C_DIM doivent √™tre d√©finis.\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ----- G√©n√©rateur conditionn√© : z' = z + M(c) -----\n",
    "class CondGenerator(nn.Module):\n",
    "    def __init__(self, G_base: nn.Module, z_dim: int, c_dim: int):\n",
    "        super().__init__()\n",
    "        self.G = G_base\n",
    "        self.z_dim = z_dim\n",
    "        self.c_dim = c_dim\n",
    "        # petit mapper M: c -> R^{z_dim}\n",
    "        self.map = nn.Sequential(\n",
    "            nn.Linear(c_dim, max(128, z_dim)), nn.SiLU(),\n",
    "            nn.Linear(max(128, z_dim), z_dim)\n",
    "        )\n",
    "    def forward(self, z: torch.Tensor, c: torch.Tensor, **kwargs):\n",
    "        # z: (B, Z_DIM), c: (B, C_DIM)\n",
    "        z_mod = z + self.map(c)\n",
    "        return self.G(z_mod, **kwargs)\n",
    "\n",
    "# ----- Discriminateur conditionn√© : projection trick -----\n",
    "class CondDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    logits = D(x) + <h(x), Wc>\n",
    "    Si D ne fournit pas de features, on fabrique h(x) via une petite t√™te hproj().\n",
    "    \"\"\"\n",
    "    def __init__(self, D_base: nn.Module, c_dim: int, feat_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.D = D_base\n",
    "        self.c_dim = c_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        # t√™te pour extraire h(x) √† partir de l'image (faible co√ªt, non invasive)\n",
    "        self.hproj = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=1, bias=False),\n",
    "            nn.SiLU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32, feat_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        # projection conditionnelle W c\n",
    "        self.proj = nn.Linear(c_dim, feat_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, c: torch.Tensor):\n",
    "        base = self.D(x)                # (B,) ou (B,1)\n",
    "        if base.ndim == 2:\n",
    "            base = base.squeeze(1)\n",
    "        h = self.hproj(x)               # (B, feat_dim)\n",
    "        wc = self.proj(c)               # (B, feat_dim)\n",
    "        return base + (h * wc).sum(dim=1)\n",
    "\n",
    "# ----- Instanciation -----\n",
    "Gc = CondGenerator(G, z_dim=Z_DIM, c_dim=C_DIM).to(DEVICE)\n",
    "Dc = CondDiscriminator(D, c_dim=C_DIM, feat_dim=256).to(DEVICE)\n",
    "\n",
    "# ----- Sanity check rapide -----\n",
    "with torch.no_grad():\n",
    "    B = 4\n",
    "    z = torch.randn(B, Z_DIM, device=DEVICE)\n",
    "    x_fake = Gc(z, torch.randn(B, C_DIM, device=DEVICE)).clamp(-1, 1)  # suppos√© [-1,1] dans ton pipeline\n",
    "    _ = Dc(x_fake, torch.randn(B, C_DIM, device=DEVICE))\n",
    "print(f\"[Wrappers] OK ‚Üí Gc(z,c)‚Üí{tuple(x_fake.shape)} ; Dc(x,c)‚Üí(B,)\")\n",
    "\n",
    "# ----- (option) Politiques de gel pour l'entra√Ænement 'lite' (on g√®le presque tout sauf les petites t√™tes) -----\n",
    "def set_requires_grad(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad_(flag)\n",
    "\n",
    "# G√®le G et D d'origine\n",
    "set_requires_grad(G, False)\n",
    "set_requires_grad(D, False)\n",
    "\n",
    "# Autorise l'entra√Ænement des parties ajout√©es (mapper de Gc, hproj & proj de Dc)\n",
    "for p in Gc.map.parameters(): p.requires_grad_(True)\n",
    "for p in Dc.hproj.parameters(): p.requires_grad_(True)\n",
    "for p in Dc.proj.parameters(): p.requires_grad_(True)\n",
    "\n",
    "print(\"[Freeze] G & D gel√©s ; t√™tes conditionnelles (Gc.map, Dc.hproj, Dc.proj) entra√Ænables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21b46b-56ea-45ca-aae7-359c24a255c3",
   "metadata": {},
   "source": [
    "### üß™ Cellule test ‚Äî smoke test direct + (si pr√©sent) test avec Gc/Dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3115df6-a4c3-4d17-b3ee-e56e41fc4cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAW] G(z)->(4, 3, 128, 128) in [-1.00,1.00] | D(x)->(4,)\n",
      "[WRAPPED] Gc(z,c)->(4, 3, 128, 128) | Dc(x,c)->(4,)\n"
     ]
    }
   ],
   "source": [
    "# === CELL GD-2 ‚Äî Smoke tests (brut + via wrappers si d√©j√† d√©finis) ===\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "B = 4\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(B, Z_DIM, device=DEVICE)\n",
    "    x_fake = G(z).clamp(-1,1)\n",
    "    d_out  = D(x_fake)\n",
    "print(f\"[RAW] G(z)->{tuple(x_fake.shape)} in [{x_fake.min().item():.2f},{x_fake.max().item():.2f}] | D(x)->{tuple(d_out.shape)}\")\n",
    "\n",
    "# Test via wrappers (Cell 5) si d√©j√† d√©finis\n",
    "if 'Gc' in globals() and 'Dc' in globals() and 'cond_from_proto' in globals():\n",
    "    with torch.no_grad():\n",
    "        # fabrique un c bidon si pas de PROTOS (sinon utilise un vrai)\n",
    "        if 'PROTOS' in globals() and len(PROTOS)>0:\n",
    "            labels = torch.tensor([0]*B, device=DEVICE)  # 0 = premi√®re classe du mapping\n",
    "            c = cond_from_proto(labels, noise_std=0.0)\n",
    "        else:\n",
    "            C_DIM = globals().get(\"C_DIM\", 128)\n",
    "            c = torch.randn(B, C_DIM, device=DEVICE)\n",
    "\n",
    "        z = torch.randn(B, Z_DIM, device=DEVICE)\n",
    "        x_fake2 = Gc(z, c).clamp(-1,1)\n",
    "        d_out2  = Dc(x_fake2, c)\n",
    "    print(f\"[WRAPPED] Gc(z,c)->{tuple(x_fake2.shape)} | Dc(x,c)->{tuple(d_out2.shape)}\")\n",
    "else:\n",
    "    print(\"[WRAPPED] Wrappers Gc/Dc non trouv√©s (ex√©cute la Cellule 5 pour les cr√©er).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dfba2-6fa6-4231-97e4-4fe9974bdef5",
   "metadata": {},
   "source": [
    "## üß± CELL 6 ‚Äî Entra√Ænement \"Lite\" (Adapter + t√™tes conditionnelles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73cb0fb-2d2e-4ee9-8c6d-bd97d1daf71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ RUN_ID=20251030_224524\n",
      "üìÅ TensorBoard: /workspace/runs_pixcell/20251030_224524\n",
      "üìÑ CSV: train_log_20251030_224524.csv\n"
     ]
    }
   ],
   "source": [
    "# === CELL 6-LOGGER ‚Äî TensorBoard + CSV (un par run) + trac√©s Matplotlib ===\n",
    "import os, time, json, math, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Dossiers\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\", \".\")).resolve()\n",
    "ARTI_DIR     = (PROJECT_ROOT / \"artifacts_pixcell\").resolve(); ARTI_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR     = (PROJECT_ROOT / \"runs_pixcell\").resolve();       RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Identifiant de run (un CSV par run)\n",
    "RUN_ID   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "TB_DIR   = RUNS_DIR / RUN_ID\n",
    "CSV_PATH = ARTI_DIR / f\"train_log_{RUN_ID}.csv\"\n",
    "print(f\"üß™ RUN_ID={RUN_ID}\\nüìÅ TensorBoard: {TB_DIR}\\nüìÑ CSV: {CSV_PATH.name}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=str(TB_DIR))\n",
    "_history = []  # liste de dicts\n",
    "\n",
    "def log_metrics(step:int,\n",
    "                d_loss:float=None, g_loss:float=None,\n",
    "                r1:float=None,\n",
    "                ada_p:float=None, ada_acc:float=None,\n",
    "                lrD:float=None, lrG:float=None, lrA:float=None):\n",
    "    \"\"\"Enregistre dans TensorBoard + m√©moire; le CSV est (r√©)√©crit p√©riodiquement.\"\"\"\n",
    "    row = {\"step\": step, \"time_s\": time.time()}\n",
    "    if d_loss is not None: row[\"loss_D\"] = float(d_loss); writer.add_scalar(\"loss/D\", d_loss, step)\n",
    "    if g_loss is not None: row[\"loss_G\"] = float(g_loss); writer.add_scalar(\"loss/G\", g_loss, step)\n",
    "    if r1     is not None: row[\"loss_R1\"] = float(r1);    writer.add_scalar(\"reg/R1\", r1, step)\n",
    "    if ada_p  is not None: row[\"ADA_p\"]   = float(ada_p); writer.add_scalar(\"ADA/p\", ada_p, step)\n",
    "    if ada_acc is not None:row[\"ADA_acc\"] = float(ada_acc);writer.add_scalar(\"ADA/acc_ema\", ada_acc, step)\n",
    "    if lrD is not None:    row[\"lr_D\"]    = float(lrD);   writer.add_scalar(\"lr/D\", lrD, step)\n",
    "    if lrG is not None:    row[\"lr_G\"]    = float(lrG);   writer.add_scalar(\"lr/G\", lrG, step)\n",
    "    if lrA is not None:    row[\"lr_A\"]    = float(lrA);   writer.add_scalar(\"lr/A\", lrA, step)\n",
    "\n",
    "    _history.append(row)\n",
    "\n",
    "def flush_history(to_csv_every:int=200, force:bool=False):\n",
    "    \"\"\"√âcrit/actualise le CSV unique de ce run toutes les N it√©rations (ou forc√© en fin de run).\"\"\"\n",
    "    if (not force) and (len(_history) % to_csv_every): \n",
    "        return\n",
    "    if not _history:\n",
    "        return\n",
    "    df = pd.DataFrame(_history).sort_values(\"step\")\n",
    "    df.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "def plot_training():\n",
    "    \"\"\"Trace rapide des courbes depuis la m√©moire (_history).\"\"\"\n",
    "    if not _history:\n",
    "        print(\"Pas encore d'historique.\")\n",
    "        return\n",
    "    df = pd.DataFrame(_history).sort_values(\"step\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    # Losses\n",
    "    axes[0,0].plot(df[\"step\"], df.get(\"loss_D\", pd.Series()), label=\"D\")\n",
    "    axes[0,0].plot(df[\"step\"], df.get(\"loss_G\", pd.Series()), label=\"G\")\n",
    "    axes[0,0].set_title(\"Losses (D / G)\"); axes[0,0].legend(); axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # R1\n",
    "    if \"loss_R1\" in df:\n",
    "        axes[0,1].plot(df[\"step\"], df[\"loss_R1\"])\n",
    "        axes[0,1].set_title(\"R1 penalty\"); axes[0,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[0,1].axis(\"off\")\n",
    "\n",
    "    # ADA\n",
    "    if \"ADA_p\" in df or \"ADA_acc\" in df:\n",
    "        if \"ADA_p\" in df:   axes[1,0].plot(df[\"step\"], df[\"ADA_p\"],  label=\"p\")\n",
    "        if \"ADA_acc\" in df: axes[1,0].plot(df[\"step\"], df[\"ADA_acc\"], label=\"acc_ema\")\n",
    "        axes[1,0].set_title(\"ADA\"); axes[1,0].legend(); axes[1,0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,0].axis(\"off\")\n",
    "\n",
    "    # LRs\n",
    "    has_lr = any(k in df for k in (\"lr_D\",\"lr_G\",\"lr_A\"))\n",
    "    if has_lr:\n",
    "        if \"lr_D\" in df: axes[1,1].plot(df[\"step\"], df[\"lr_D\"], label=\"lr_D\")\n",
    "        if \"lr_G\" in df: axes[1,1].plot(df[\"step\"], df[\"lr_G\"], label=\"lr_G\")\n",
    "        if \"lr_A\" in df: axes[1,1].plot(df[\"step\"], df[\"lr_A\"], label=\"lr_A\")\n",
    "        axes[1,1].set_title(\"Learning Rates\"); axes[1,1].legend(); axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "    print(f\"‚û°Ô∏è Ouvre TensorBoard pour plus de d√©tails : tensorboard --logdir {RUNS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d440ca6-4e15-431d-b096-1f8fa035eff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119/2979164981.py:88: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_D = GradScaler(enabled=USE_AMP)\n",
      "/tmp/ipykernel_119/2979164981.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler_G = GradScaler(enabled=USE_AMP)\n",
      "/tmp/ipykernel_119/2979164981.py:110: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP):\n",
      "/tmp/ipykernel_119/2979164981.py:129: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP):\n",
      "/tmp/ipykernel_119/2979164981.py:145: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=USE_AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00000] D=1.381 G=0.708 | 0.0 min\n",
      "üñºÔ∏è samples ‚Üí /workspace/samples/pixcell_lite_fix_step000000.png\n",
      "[00200] D=2.010 G=0.689 | 0.7 min\n",
      "[00400] D=1.868 G=0.302 | 1.3 min\n",
      "üñºÔ∏è samples ‚Üí /workspace/samples/pixcell_lite_fix_step000500.png\n",
      "[00600] D=1.860 G=0.796 | 2.0 min\n",
      "[00800] D=1.783 G=0.353 | 2.7 min\n",
      "[01000] D=1.553 G=0.403 | 3.4 min\n",
      "üñºÔ∏è samples ‚Üí /workspace/samples/pixcell_lite_fix_step001000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 244, in run\n",
      "    self._run()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py\", line 275, in _run\n",
      "    self._record_writer.write(data)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/record_writer.py\", line 40, in write\n",
      "    self._writer.write(header + header_crc + data + footer_crc)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 775, in write\n",
      "    self.fs.append(self.filename, file_content, self.binary_mode)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 167, in append\n",
      "    self._write(filename, file_content, \"ab\" if binary_mode else \"a\")\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py\", line 171, in _write\n",
      "    with io.open(filename, mode, encoding=encoding) as f:\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 9] Bad file descriptor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m scaler_D\u001b[38;5;241m.\u001b[39munscale_(OPT_D); nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(params_D, CLIP_D)\n\u001b[1;32m    118\u001b[0m scaler_D\u001b[38;5;241m.\u001b[39mstep(OPT_D); scaler_D\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m--> 119\u001b[0m log_metrics(global_step, d_loss\u001b[38;5;241m=\u001b[39mloss_D\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    120\u001b[0m             ada_p\u001b[38;5;241m=\u001b[39m(ADA_STATE\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADA_STATE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    121\u001b[0m             ada_acc\u001b[38;5;241m=\u001b[39m(ADA_STATE\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc_ema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADA_STATE\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    122\u001b[0m             lrD\u001b[38;5;241m=\u001b[39mOPT_D\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    123\u001b[0m flush_history()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# R1 p√©riodique\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mlog_metrics\u001b[0;34m(step, d_loss, g_loss, r1, ada_p, ada_acc, lrD, lrG, lrA)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Enregistre dans TensorBoard + m√©moire; le CSV est (r√©)√©crit p√©riodiquement.\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: step, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_s\u001b[39m\u001b[38;5;124m\"\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime()}\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_D\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(d_loss); writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss/D\u001b[39m\u001b[38;5;124m\"\u001b[39m, d_loss, step)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m g_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_G\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(g_loss); writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss/G\u001b[39m\u001b[38;5;124m\"\u001b[39m, g_loss, step)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r1     \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_R1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(r1);    writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg/R1\u001b[39m\u001b[38;5;124m\"\u001b[39m, r1, step)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:381\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[0;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[1;32m    376\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_scalar\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    378\u001b[0m summary \u001b[38;5;241m=\u001b[39m scalar(\n\u001b[1;32m    379\u001b[0m     tag, scalar_value, new_style\u001b[38;5;241m=\u001b[39mnew_style, double_precision\u001b[38;5;241m=\u001b[39mdouble_precision\n\u001b[1;32m    380\u001b[0m )\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:115\u001b[0m, in \u001b[0;36mFileWriter.add_summary\u001b[0;34m(self, summary, global_step, walltime)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a `Summary` protocol buffer to the event file.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mThis method wraps the provided summary in an `Event` protocol buffer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    walltime (from time.time()) seconds after epoch\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m event \u001b[38;5;241m=\u001b[39m event_pb2\u001b[38;5;241m.\u001b[39mEvent(summary\u001b[38;5;241m=\u001b[39msummary)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_event(event, global_step, walltime)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/tensorboard/writer.py:99\u001b[0m, in \u001b[0;36mFileWriter.add_event\u001b[0;34m(self, event, step, walltime)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# Make sure step is converted from numpy or other formats\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# since protobuf might not convert depending on version\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     event\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(step)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_writer\u001b[38;5;241m.\u001b[39madd_event(event)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:117\u001b[0m, in \u001b[0;36mEventFileWriter.add_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, event_pb2\u001b[38;5;241m.\u001b[39mEvent):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an event_pb2.Event proto, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(event)\n\u001b[1;32m    116\u001b[0m     )\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_writer\u001b[38;5;241m.\u001b[39mwrite(event\u001b[38;5;241m.\u001b[39mSerializeToString())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:171\u001b[0m, in \u001b[0;36m_AsyncWriter.write\u001b[0;34m(self, bytestring)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Enqueue the given bytes to be written asychronously.\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Status of the worker should be checked under the lock to avoid\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# multiple threads passing the check and then switching just before\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# blocking on putting to the queue which might result in a deadlock.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_worker_status()\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWriter is closed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:212\u001b[0m, in \u001b[0;36m_AsyncWriter._check_worker_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker\u001b[38;5;241m.\u001b[39mexception\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:1045\u001b[0m, in \u001b[0;36mThread._bootstrap_inner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     _sys\u001b[38;5;241m.\u001b[39msetprofile(_profile_hook)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke_excepthook(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:244\u001b[0m, in \u001b[0;36m_AsyncWriterThread.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/event_file_writer.py:275\u001b[0m, in \u001b[0;36m_AsyncWriterThread._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown_signal:\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_writer\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_pending_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/summary/writer/record_writer.py:40\u001b[0m, in \u001b[0;36mRecordWriter.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     38\u001b[0m header_crc \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, masked_crc32c(header))\n\u001b[1;32m     39\u001b[0m footer_crc \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, masked_crc32c(data))\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mwrite(header \u001b[38;5;241m+\u001b[39m header_crc \u001b[38;5;241m+\u001b[39m data \u001b[38;5;241m+\u001b[39m footer_crc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:775\u001b[0m, in \u001b[0;36mGFile.write\u001b[0;34m(self, file_content)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_started \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# append the later chunks\u001b[39;00m\n\u001b[0;32m--> 775\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename, file_content, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary_mode)\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# add to temp file, but wait for flush to write to final filesystem\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_temp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:167\u001b[0m, in \u001b[0;36mLocalFileSystem.append\u001b[0;34m(self, filename, file_content, binary_mode)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mappend\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, binary_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Append string file contents to a file.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m        binary_mode: bool, write as binary if True, otherwise text\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write(filename, file_content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m binary_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorboard/compat/tensorflow_stub/io/gfile.py:171\u001b[0m, in \u001b[0;36mLocalFileSystem._write\u001b[0;34m(self, filename, file_content, mode)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename, file_content, mode):\n\u001b[1;32m    170\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(filename, mode, encoding\u001b[38;5;241m=\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    172\u001b[0m         compatify \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_bytes \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;28;01melse\u001b[39;00m compat\u001b[38;5;241m.\u001b[39mas_text\n\u001b[1;32m    173\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(compatify(file_content))\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 9] Bad file descriptor"
     ]
    }
   ],
   "source": [
    "# === CELL 6 ‚Äî FINAL (Lite training stabilis√©) ===\n",
    "import time, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from pathlib import Path\n",
    "\n",
    "assert {'Gc','Dc','G','D','adapter','cond_from_proto','ds_gan'} <= set(globals()), \\\n",
    "    \"Pr√©-requis manquants (ex√©cute les Cellules 3‚Äì5).\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROJECT_ROOT = Path(os.getenv(\"PROJECT_ROOT\", \".\")).resolve()\n",
    "SAMPLES_DIR  = (PROJECT_ROOT / \"samples\").resolve(); SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------\n",
    "# 1) z' = z + Œ±*M(c) (r√©siduel stable)\n",
    "# --------------------------\n",
    "RES_SCALE = 0.1\n",
    "def _forward_gc_residual(self, z, c, **kw):\n",
    "    return self.G(z + RES_SCALE * self.map(c), **kw)\n",
    "Gc.forward = _forward_gc_residual.__get__(Gc, type(Gc))\n",
    "\n",
    "# --------------------------\n",
    "# 2) Spectral norm sur Dc.hproj / Dc.proj\n",
    "# --------------------------\n",
    "def _apply_sn(module):\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            try: torch.nn.utils.spectral_norm(m)\n",
    "            except Exception: pass\n",
    "_apply_sn(Dc.hproj); _apply_sn(Dc.proj)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Politique de gel (l√©ger d√©gel utile)\n",
    "# --------------------------\n",
    "def set_requires_grad(m, flag):\n",
    "    for p in m.parameters(): p.requires_grad_(flag)\n",
    "\n",
    "set_requires_grad(G, False); set_requires_grad(D, False)          # g√®le les bases\n",
    "set_requires_grad(Gc.map, True); set_requires_grad(Dc.hproj, True); set_requires_grad(Dc.proj, True)\n",
    "if hasattr(G, \"ups\") and len(G.ups)>0: set_requires_grad(G.ups[-1], True)\n",
    "if hasattr(G, \"to_rgb\"):                set_requires_grad(G.to_rgb, True)\n",
    "if hasattr(D, \"fc\"):                    set_requires_grad(D.fc, True)\n",
    "\n",
    "# --------------------------\n",
    "# 4) DataLoader (conforme √† ton contexte)\n",
    "# --------------------------\n",
    "BATCH_SIZE  = globals().get(\"BATCH_SIZE\", 16)\n",
    "Z_DIM       = globals().get(\"Z_DIM\", 512)\n",
    "C_DIM       = globals().get(\"C_DIM\", 128)\n",
    "USE_AMP     = globals().get(\"USE_AMP\", True)\n",
    "R1_EVERY    = globals().get(\"R1_EVERY\", 16)\n",
    "R1_GAMMA    = globals().get(\"R1_GAMMA\", 10.0)\n",
    "CLIP_D      = globals().get(\"CLIP_D\", 5.0)\n",
    "CLIP_G      = globals().get(\"CLIP_G\", 10.0)\n",
    "STEPS_LITE  = 20_00 # nombre total d'it√©rations\n",
    "LOG_EVERY   = globals().get(\"LOG_EVERY\", 200)\n",
    "SAMPLE_EVERY= globals().get(\"SAMPLE_EVERY\", 500) # echantillons p√©riodiques\n",
    "\n",
    "train_dl_gan = DataLoader(ds_gan, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=0, pin_memory=False, drop_last=True)\n",
    "\n",
    "# --------------------------\n",
    "# 5) Pertes & optims (LRs sages)\n",
    "# --------------------------\n",
    "def d_logistic_loss(real_pred, fake_pred):\n",
    "    return F.softplus(-real_pred).mean() + F.softplus(fake_pred).mean()\n",
    "def g_nonsat_loss(fake_pred):\n",
    "    return F.softplus(-fake_pred).mean()\n",
    "\n",
    "params_D = list(Dc.hproj.parameters()) + list(Dc.proj.parameters())\n",
    "if hasattr(D, \"fc\"): params_D += list(D.fc.parameters())\n",
    "params_G = list(Gc.map.parameters())\n",
    "if hasattr(G, \"ups\") and len(G.ups)>0: params_G += list(G.ups[-1].parameters())\n",
    "if hasattr(G, \"to_rgb\"): params_G += list(G.to_rgb.parameters())\n",
    "params_A = list(adapter.parameters())\n",
    "\n",
    "OPT_D = torch.optim.Adam(params_D, lr=1e-4, betas=(0.0, 0.99))\n",
    "OPT_G = torch.optim.Adam(params_G, lr=2e-4, betas=(0.0, 0.99))\n",
    "OPT_A = torch.optim.Adam(params_A, lr=5e-4, betas=(0.0, 0.99))\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "sched_D = CosineAnnealingLR(OPT_D, T_max=STEPS_LITE, eta_min=1e-5)\n",
    "sched_G = CosineAnnealingLR(OPT_G, T_max=STEPS_LITE, eta_min=1e-5)\n",
    "sched_A = CosineAnnealingLR(OPT_A, T_max=STEPS_LITE, eta_min=1e-5)\n",
    "\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "scaler_D = GradScaler(enabled=USE_AMP)\n",
    "scaler_G = GradScaler(enabled=USE_AMP)\n",
    "\n",
    "# z fixe pour suivi visuel\n",
    "torch.manual_seed(1234)\n",
    "FIX_PER_CLASS = 8\n",
    "z_fixed = torch.randn(FIX_PER_CLASS, Z_DIM, device=DEVICE)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Boucle d'entra√Ænement ‚ÄúLite‚Äù\n",
    "# --------------------------\n",
    "global_step, t0 = 0, time.time()\n",
    "while global_step < STEPS_LITE:\n",
    "    for x_real, y_lbl, _ in train_dl_gan:\n",
    "        if global_step >= STEPS_LITE: break\n",
    "        x_real = x_real.to(DEVICE); y_lbl = y_lbl.to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            c_cond = cond_from_proto(y_lbl, noise_std=0.0)  # (B, C_DIM)\n",
    "\n",
    "        # === D\n",
    "        OPT_D.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=USE_AMP):\n",
    "            d_real = Dc(x_real, c_cond)\n",
    "            z = torch.randn(x_real.size(0), Z_DIM, device=DEVICE)\n",
    "            x_fake = Gc(z, c_cond).detach()\n",
    "            d_fake = Dc(x_fake, c_cond)\n",
    "            loss_D = d_logistic_loss(d_real, d_fake)\n",
    "        scaler_D.scale(loss_D).backward()\n",
    "        scaler_D.unscale_(OPT_D); nn.utils.clip_grad_norm_(params_D, CLIP_D)\n",
    "        scaler_D.step(OPT_D); scaler_D.update()\n",
    "        log_metrics(global_step, d_loss=loss_D.item(),\n",
    "                    ada_p=(ADA_STATE.get(\"p\") if 'ADA_STATE' in globals() else None),\n",
    "                    ada_acc=(ADA_STATE.get(\"acc_ema\") if 'ADA_STATE' in globals() else None),\n",
    "                    lrD=OPT_D.param_groups[0]['lr'])\n",
    "        flush_history()\n",
    "\n",
    "        # R1 p√©riodique\n",
    "        if (global_step % R1_EVERY) == 0:\n",
    "            OPT_D.zero_grad(set_to_none=True)\n",
    "            xr = x_real.detach().requires_grad_(True)\n",
    "            with autocast(enabled=USE_AMP):\n",
    "                drr = Dc(xr, c_cond)\n",
    "                grad = torch.autograd.grad(drr.sum(), xr, create_graph=True)[0]\n",
    "                r1  = 0.5 * R1_GAMMA * grad.pow(2).view(grad.size(0), -1).sum(1).mean()\n",
    "            scaler_D.scale(r1).backward()\n",
    "            scaler_D.unscale_(OPT_D); nn.utils.clip_grad_norm_(params_D, CLIP_D)\n",
    "            scaler_D.step(OPT_D); scaler_D.update()\n",
    "            log_metrics(global_step, r1=r1.item() if 'r1' in locals() else None)\n",
    "            flush_history()\n",
    "\n",
    "\n",
    "        if 'ada_update' in globals():\n",
    "            with torch.no_grad(): ada_update(d_real.detach())\n",
    "\n",
    "        # === G + Adapter\n",
    "        OPT_G.zero_grad(set_to_none=True); OPT_A.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=USE_AMP):\n",
    "            e_batch = torch.stack([PROTOS[int(k.item())].to(DEVICE) for k in y_lbl], 0)\n",
    "            c_train = adapter(e_batch)\n",
    "            z = torch.randn(x_real.size(0), Z_DIM, device=DEVICE)\n",
    "            xf = Gc(z, c_train)\n",
    "            dfg = Dc(xf, c_train)\n",
    "            loss_G = g_nonsat_loss(dfg)\n",
    "        scaler_G.scale(loss_G).backward()\n",
    "        scaler_G.unscale_(OPT_G); scaler_G.unscale_(OPT_A)\n",
    "        nn.utils.clip_grad_norm_(params_G, CLIP_G); nn.utils.clip_grad_norm_(params_A, CLIP_G)\n",
    "        scaler_G.step(OPT_G); scaler_G.step(OPT_A); scaler_G.update()\n",
    "        # Step des schedulers\n",
    "        sched_D.step(); sched_G.step(); sched_A.step()\n",
    "        log_metrics(global_step, g_loss=loss_G.item(),\n",
    "                    lrG=OPT_G.param_groups[0]['lr'], lrA=OPT_A.param_groups[0]['lr'])\n",
    "        flush_history()\n",
    "\n",
    "\n",
    "        # logs & samples\n",
    "        if (global_step % LOG_EVERY) == 0:\n",
    "            print(f\"[{global_step:05d}] D={loss_D.item():.3f} G={loss_G.item():.3f} | {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "        if (global_step % SAMPLE_EVERY) == 0:\n",
    "            with torch.no_grad():\n",
    "                rows = []\n",
    "                for k in sorted(PROTOS.keys()):\n",
    "                    labels = torch.full((FIX_PER_CLASS,), int(k), dtype=torch.long, device=DEVICE)\n",
    "                    c_s = cond_from_proto(labels, noise_std=0.0)\n",
    "                    imgs = Gc(z_fixed, c_s).clamp(-1,1)\n",
    "                    rows.append(imgs)\n",
    "                grid = torch.cat(rows, 0)\n",
    "                out_path = SAMPLES_DIR / f\"pixcell_lite_fix_step{global_step:06d}.png\"\n",
    "                save_image((grid+1)/2, out_path, nrow=FIX_PER_CLASS)\n",
    "                print(f\"üñºÔ∏è samples ‚Üí {out_path}\")\n",
    "\n",
    "        if (global_step % 2000) == 0 and global_step > 0:\n",
    "            ckpt = {\n",
    "                \"step\": global_step,\n",
    "                \"Gc_map\": Gc.map.state_dict(),\n",
    "                \"Dc_hproj\": Dc.hproj.state_dict(),\n",
    "                \"Dc_proj\": Dc.proj.state_dict(),\n",
    "                \"G_tail\": getattr(G, \"to_rgb\", nn.Identity()).state_dict() if hasattr(G,\"to_rgb\") else {},\n",
    "                \"D_fc\":   getattr(D, \"fc\", nn.Identity()).state_dict() if hasattr(D,\"fc\") else {},\n",
    "                \"adapter\": adapter.state_dict(),\n",
    "                \"optD\": OPT_D.state_dict(), \"optG\": OPT_G.state_dict(), \"optA\": OPT_A.state_dict(),\n",
    "                \"schD\": sched_D.state_dict(), \"schG\": sched_G.state_dict(), \"schA\": sched_A.state_dict(),\n",
    "            }\n",
    "            torch.save(ckpt, SAMPLES_DIR / f\"pixcell_lite_step{global_step:06d}.pt\")\n",
    "\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "print(\"‚úÖ Entra√Ænement Lite (stabilis√©) termin√©.\")\n",
    "flush_history(force=True)\n",
    "plot_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa146907-3ae0-4a72-b4f2-d48af8a9a359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
