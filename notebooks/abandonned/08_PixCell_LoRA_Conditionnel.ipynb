{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71780fa-9544-434f-b9ce-9de62e486db6",
   "metadata": {},
   "source": [
    "# üß© Cellule 1 ‚Äì Environnement et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff0ba59-92ca-4416-ad05-ad200ea3ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device actif : cuda\n",
      "üìÇ Dossier de sortie : /workspace/outputs/pixcell_lora\n",
      "GPU d√©tect√© : NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version : 12.4\n",
      "PyTorch version : 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# üß© 08_PixCell_LoRA_Conditionnel.ipynb\n",
    "# Cellule 1 : Imports & configuration de l'environnement\n",
    "# ===============================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîß Configuration g√©n√©rale\n",
    "# ---------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"‚úÖ Device actif : {device}\")\n",
    "\n",
    "# R√©pertoires du projet\n",
    "PROJECT_ROOT = Path(\"/workspace\")  # adapte selon ton environnement\n",
    "DATA_DIR     = PROJECT_ROOT / \"data\" / \"NCT-CRC-HE-100K\"\n",
    "CONFIG_DIR   = PROJECT_ROOT / \"configs\"\n",
    "OUTPUT_DIR   = PROJECT_ROOT / \"outputs\" / \"pixcell_lora\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Dossier de sortie : {OUTPUT_DIR}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# V√©rification CUDA\n",
    "# ---------------------------------------------------------------\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU d√©tect√© : {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version : {torch.version.cuda}\")\n",
    "    print(f\"PyTorch version : {torch.__version__}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pas de GPU d√©tect√© ‚Äî attention aux performances.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9331dc-90ec-4253-af85-004138301483",
   "metadata": {},
   "source": [
    "# üß© Cellule 2 ‚Äì Chargement de PixCell + UNI-2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387422cb-fc75-49fd-a2fa-c50b5a19def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "üîë Authentification Hugging Face r√©ussie.\n",
      "‚è≥ Chargement du VAE (SD 3.5 Large)‚Ä¶\n",
      "‚úÖ VAE charg√©.\n",
      "‚è≥ Chargement de PixCell-256‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'trust_remote_code': True} are not expected by PixCellPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbfa51ccfab4a46ad5ab725439313cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'flow_shift': 1.0, 'use_flow_sigmas': False} were passed to DPMSolverMultistepScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "The config attributes {'double_self_attention': False, 'num_vector_embeds': None, 'only_cross_attention': False, 'use_linear_projection': False} were passed to PixCellTransformer2DModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PixCell-256 charg√©.\n",
      "\n",
      "‚è≥ Chargement du mod√®le UNI-2h (via timm)‚Ä¶\n",
      "‚úÖ UNI-2h pr√™t ‚Üí torch.float32 | device: cuda\n",
      "\n",
      "PixCell dtype : torch.float16, device : cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# üß© Cellule 2 : Chargement PixCell (avec VAE explicite) + UNI-2h (timm)\n",
    "# ===============================================================\n",
    "import os, shutil, torch, timm\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login\n",
    "from diffusers import DiffusionPipeline, AutoencoderKL\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîê Authentification Hugging Face\n",
    "# ---------------------------------------------------------------\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "assert HF_TOKEN is not None, \"‚ö†Ô∏è Variable d‚Äôenvironnement HF_TOKEN absente.\"\n",
    "login(token=HF_TOKEN, add_to_git_credential=True)\n",
    "print(\"üîë Authentification Hugging Face r√©ussie.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CACHE_DIR = Path(os.getenv(\"HF_HOME\", \"/workspace/.cache/huggingface\"))\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Chargement du pipeline PixCell-256 (avec VAE explicite)\n",
    "# ---------------------------------------------------------------\n",
    "def load_pixcell_with_explicit_vae(device):\n",
    "    print(\"‚è≥ Chargement du VAE (SD 3.5 Large)‚Ä¶\")\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-3.5-large\",\n",
    "        subfolder=\"vae\",\n",
    "        torch_dtype=torch.float16,\n",
    "        token=HF_TOKEN,\n",
    "    ).to(device)\n",
    "    print(\"‚úÖ VAE charg√©.\")\n",
    "\n",
    "    print(\"‚è≥ Chargement de PixCell-256‚Ä¶\")\n",
    "    pipe = DiffusionPipeline.from_pretrained(\n",
    "        \"StonyBrook-CVLab/PixCell-256\",\n",
    "        custom_pipeline=\"StonyBrook-CVLab/PixCell-pipeline\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        vae=vae,\n",
    "        token=HF_TOKEN,\n",
    "    ).to(device)\n",
    "    print(\"‚úÖ PixCell-256 charg√©.\")\n",
    "    return pipe\n",
    "\n",
    "try:\n",
    "    pipe256 = load_pixcell_with_explicit_vae(device)\n",
    "except OSError as e:\n",
    "    print(\"‚ö†Ô∏è Cache local corrompu :\", e)\n",
    "    model_cache_root = CACHE_DIR / \"hub\" / \"models--StonyBrook-CVLab--PixCell-256\" / \"snapshots\"\n",
    "    if model_cache_root.exists():\n",
    "        for snap in model_cache_root.iterdir():\n",
    "            vae_dir = snap / \"vae\"\n",
    "            if vae_dir.exists():\n",
    "                print(f\"üßπ Suppression du sous-dossier VAE corrompu : {vae_dir}\")\n",
    "                shutil.rmtree(vae_dir, ignore_errors=True)\n",
    "    pipe256 = load_pixcell_with_explicit_vae(device)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Chargement du mod√®le UNI-2h (timm)\n",
    "# ---------------------------------------------------------------\n",
    "print(\"\\n‚è≥ Chargement du mod√®le UNI-2h (via timm)‚Ä¶\")\n",
    "\n",
    "timm_kwargs = {\n",
    "    \"img_size\": 224,\n",
    "    \"patch_size\": 14,\n",
    "    \"depth\": 24,\n",
    "    \"num_heads\": 24,\n",
    "    \"embed_dim\": 1536,\n",
    "    \"mlp_ratio\": 2.66667 * 2,   # sp√©cifique √† UNI-2h\n",
    "    \"init_values\": 1e-5,\n",
    "    \"num_classes\": 0,\n",
    "    \"no_embed_class\": True,\n",
    "    \"reg_tokens\": 8,\n",
    "    \"dynamic_img_size\": True,\n",
    "    \"mlp_layer\": timm.layers.SwiGLUPacked,\n",
    "    \"act_layer\": torch.nn.SiLU,\n",
    "}\n",
    "\n",
    "uni = timm.create_model(\n",
    "    \"hf-hub:MahmoodLab/UNI2-h\",\n",
    "    pretrained=True,\n",
    "    **timm_kwargs\n",
    ").eval().to(device)\n",
    "\n",
    "cfg = resolve_data_config(uni.pretrained_cfg, model=uni)\n",
    "tfm = create_transform(**cfg)\n",
    "\n",
    "print(\"‚úÖ UNI-2h pr√™t ‚Üí\", next(uni.parameters()).dtype, \"| device:\", device)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Fonction utilitaire : embeddings UNI-2h\n",
    "# ---------------------------------------------------------------\n",
    "@torch.inference_mode()\n",
    "def uni_embed_from_tensor(x_3chw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extrait les embeddings UNI-2h d'une image (B,3,H,W)\n",
    "    ‚Üí Retourne (B,1536) en float32\n",
    "    \"\"\"\n",
    "    if x_3chw.shape[-1] != 224:\n",
    "        x_3chw = F.interpolate(x_3chw, size=(224, 224),\n",
    "                               mode=\"bilinear\", align_corners=False)\n",
    "    e = uni(x_3chw).float()\n",
    "    if hasattr(e, \"shape\") and e.ndim == 3:\n",
    "        e = e[:, 0]  # token [CLS]\n",
    "    return e\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ V√©rification dtype/device PixCell\n",
    "# ---------------------------------------------------------------\n",
    "ref_weight = pipe256.transformer.pos_embed.proj.weight\n",
    "PIPE_DTYPE = ref_weight.dtype\n",
    "PIPE_DEV   = ref_weight.device\n",
    "print(f\"\\nPixCell dtype : {PIPE_DTYPE}, device : {PIPE_DEV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ac5c4-93d5-43c4-a4bd-f9a2dea048cf",
   "metadata": {},
   "source": [
    "# üß© Cellule 3 ‚Äì Dataset, mappings et prototypes UNI-2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5726323c-c44d-490c-8082-3d0f05acb672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chemin ajout√© au PYTHONPATH : /workspace\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# ‚öôÔ∏è Configuration du chemin projet pour importer p9dg\n",
    "# ===============================================================\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\"/workspace\")  # üîß adapte si besoin (par ex. \"..\" en local)\n",
    "PACKAGE_DIR = PROJECT_ROOT / \"p9dg\"\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "assert PACKAGE_DIR.exists(), f\"‚ùå Dossier {PACKAGE_DIR} introuvable.\"\n",
    "print(f\"‚úÖ Chemin ajout√© au PYTHONPATH : {PROJECT_ROOT}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe89623-6a8a-498d-926c-b379c17b179c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Split d√©tect√© : NCT-CRC-HE-100K\n",
      "üìÅ Base effective : /workspace/data/NCT-CRC-HE-100K\n",
      "üé® R√©f√©rence Vahadane fix√©e : TUM-TCGA-WAEEFPKC.tif\n",
      "üé® R√©f√©rence Vahadane auto: TUM-TCGA-WAEEFPKC.tif\n",
      "‚úÖ Seuils par classe charg√©s depuis : /workspace/configs/seuils_par_classe.json\n",
      "‚öñÔ∏è √âchantillonnage √©quilibr√© activ√© (30 images / classe).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'HistoDataset' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 27\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÅ Base effective : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_ROOT\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mSPLIT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m HistoDataset(\n\u001b[1;32m     19\u001b[0m     root_data\u001b[38;5;241m=\u001b[39mDATA_ROOT,\n\u001b[1;32m     20\u001b[0m     split\u001b[38;5;241m=\u001b[39mSPLIT,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     samples_per_class_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Dataset charg√© (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images) | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_ds\u001b[38;5;241m.\u001b[39mclasses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClasses :\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_ds\u001b[38;5;241m.\u001b[39mclasses)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HistoDataset' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "# ‚úÖ D√©tection robuste du split + init dataset sans doublon de chemin\n",
    "from pathlib import Path\n",
    "from p9dg.histo_dataset import HistoDataset\n",
    "\n",
    "DATA_ROOT = Path(\"/workspace/data\")            # <-- parent, pas le sous-dossier\n",
    "assert DATA_ROOT.exists(), f\"{DATA_ROOT} introuvable\"\n",
    "\n",
    "if (DATA_ROOT / \"NCT-CRC-HE-100K\").is_dir():\n",
    "    SPLIT = \"NCT-CRC-HE-100K\"\n",
    "elif (DATA_ROOT / \"CRC-VAL-HE-7K\").is_dir():\n",
    "    SPLIT = \"CRC-VAL-HE-7K\"\n",
    "else:\n",
    "    raise FileNotFoundError(\"Ni 'NCT-CRC-HE-100K' ni 'CRC-VAL-HE-7K' trouv√©s sous /workspace/data\")\n",
    "\n",
    "print(f\"üîé Split d√©tect√© : {SPLIT}\")\n",
    "print(f\"üìÅ Base effective : {DATA_ROOT / SPLIT}\")\n",
    "\n",
    "train_ds = HistoDataset(\n",
    "    root_data=DATA_ROOT,\n",
    "    split=SPLIT,\n",
    "    apply_quality_filter=True,\n",
    "    thresholds_json_path=SEUILS_PATH,   # chemin JSON (pas un dict)\n",
    "    balance_per_class=True,\n",
    "    samples_per_class_per_epoch=30,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√© ({len(train_ds)} images) | {len(train_ds.classes)} classes\")\n",
    "print(\"Classes :\", train_ds.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f8ab1-ddde-49f8-9110-586a04bec90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# üß© Cellule 3 : Chargement du dataset + mappings + prototypes UNI-2h\n",
    "# ===============================================================\n",
    "from pathlib import Path\n",
    "import json, torch\n",
    "from p9dg.histo_dataset import HistoDataset\n",
    "from utils.class_mappings import class_labels, make_idx_mappings\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Chargement des seuils par classe\n",
    "# ---------------------------------------------------------------\n",
    "SEUILS_PATH = CONFIG_DIR / \"seuils_par_classe.json\"\n",
    "with open(SEUILS_PATH, \"r\") as f:\n",
    "    seuils_par_classe = json.load(f)\n",
    "print(f\"‚úÖ Seuils par classe charg√©s depuis : {SEUILS_PATH}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Initialisation du dataset √©quilibr√©\n",
    "# ---------------------------------------------------------------\n",
    "DATA_PATH = Path(\"/workspace/data/NCT-CRC-HE-100K\")  # üîß chemin correct\n",
    "\n",
    "train_ds = HistoDataset(\n",
    "    root_data=DATA_PATH,\n",
    "    apply_quality_filter=True,\n",
    "    thresholds_json_path=SEUILS_PATH,        # chemin JSON (pas dict)\n",
    "    balance_per_class=True,\n",
    "    samples_per_class_per_epoch=30,          # pour un test rapide\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√© ({len(train_ds)} images) | {len(train_ds.classes)} classes\")\n",
    "print(\"Classes :\", train_ds.classes)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Mappings lisibles\n",
    "# ---------------------------------------------------------------\n",
    "idx_to_name, idx_to_color, class_to_label = make_idx_mappings(train_ds.class_to_idx)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Calcul ou rechargement des prototypes UNI-2h\n",
    "# ---------------------------------------------------------------\n",
    "PROTOS_PATH = OUTPUT_DIR / \"prototypes_uni2h.pt\"\n",
    "if PROTOS_PATH.exists():\n",
    "    PROTOS = torch.load(PROTOS_PATH)\n",
    "    print(f\"‚úÖ Prototypes UNI-2h recharg√©s depuis {PROTOS_PATH}\")\n",
    "else:\n",
    "    print(\"‚è≥ Calcul des prototypes UNI-2h par classe...\")\n",
    "    PROTOS = {}\n",
    "    for cname in train_ds.class_to_idx.keys():\n",
    "        imgs = [train_ds._resize(train_ds._load_image(p))\n",
    "                for p in train_ds.paths_by_class[train_ds.class_to_idx[cname]][:8]]\n",
    "        batch = torch.stack([train_ds._to_tensor(img).to(device) for img in imgs])\n",
    "        e = uni_embed_from_tensor(batch)  # [B,1536]\n",
    "        PROTOS[cname] = e.mean(0).detach().cpu()\n",
    "    torch.save(PROTOS, PROTOS_PATH)\n",
    "    print(f\"‚úÖ Prototypes UNI-2h sauvegard√©s ‚Üí {PROTOS_PATH}\")\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ V√©rification rapide\n",
    "# ---------------------------------------------------------------\n",
    "for cname, v in PROTOS.items():\n",
    "    print(f\"{cname:<8} ‚Üí {tuple(v.shape)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
