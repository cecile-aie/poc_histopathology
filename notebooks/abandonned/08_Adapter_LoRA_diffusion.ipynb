{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c19e4ea-9ab3-4060-a89a-3e44c3aacda0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "# 08_Adapter_LoRA_diffusion ‚Äî UNI2‚Äëh (gel√©) ‚Üî PixCell (r√©el) *gated*\n",
    "\n",
    "**But** : r√©duire le *domain shift* observ√© sur les embeddings (UNI2‚Äëh, PathoDuet) en adaptant l√©g√®rement la pipeline PixCell via :\n",
    "- un **Adapter** MLP (projection/pack de l‚Äôembedding UNI2‚Äëh en quelques tokens de contexte) ;\n",
    "- des **LoRA** l√©g√®res inject√©es uniquement dans les **cross‚Äëattentions** du **UNet** (PixCell).\n",
    "\n",
    "**Principe** :  \n",
    "UNI2‚Äëh est gel√© et fournit un vecteur de conditionnement `z_uni`. On monkey‚Äëpatch la pipeline sans toucher aux poids de base : on concat√®ne les tokens projet√©s par l'adapter aux `encoder_hidden_states` (texte) consomm√©s par les cross‚Äëattn, et on n‚Äôentra√Æne que l‚Äôadapter + les matrices basses‚Äërangs (LoRA).\n",
    "\n",
    "> ‚ö†Ô∏è Le *monkey patch* est central : on n‚Äôalt√®re pas la signature publique de la pipeline, on accroche un hook propre √† `_encode_prompt` (SD‚Äëlike) et on append nos tokens.  \n",
    "> ‚ö†Ô∏è Param√©trage d√©licat ‚Üí toutes les dimensions sensibles sont factoris√©es dans une seule section de config.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db640b91-be40-47d6-add2-e9a202f2ac5d",
   "metadata": {},
   "source": [
    "## Cellule 1 ‚Äî Environnement & login Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fecf1aae-f63a-48a7-a394-70d662a00cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Torch=2.4.0 | CUDA=12.4 | Device=NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# ========= Cellule 1 ‚Äî Environnement & login Hugging Face =========\n",
    "# - Installe/MAJ les libs n√©cessaires (sans xformers pour √©viter les conflits CUDA)\n",
    "# - Se connecte √† Hugging Face avec un token personnel (non affich√©)\n",
    "# - Affiche un r√©cap GPU / versions pour sanity-check\n",
    "\n",
    "# %pip -qv install --upgrade diffusers transformers accelerate safetensors huggingface_hub\n",
    "\n",
    "import os, torch\n",
    "from huggingface_hub import login\n",
    "from getpass import getpass\n",
    "\n",
    "# --- Connexion Hugging Face ---\n",
    "# Option 1 : mettre le token dans la variable d'environnement HF_TOKEN avant le lancement du notebook\n",
    "# Option 2 : saisie s√©curis√©e au clavier (recommand√© si tu n'as rien export√©)\n",
    "token = os.environ.get(\"HF_TOKEN\") or getpass(\"üëâ Entrez votre Hugging Face token (il ne sera pas affich√©) : \")\n",
    "login(token=token, add_to_git_credential=False)\n",
    "\n",
    "# --- (Optionnel) acc√©l√©ration transferts ---\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# --- R√©cap environnement GPU ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cuda_ver = getattr(torch.version, \"cuda\", None)\n",
    "gpu_name = torch.cuda.get_device_name(0) if device == \"cuda\" else \"CPU\"\n",
    "print(f\"[OK] Torch={torch.__version__} | CUDA={cuda_ver} | Device={gpu_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65393c43-5ab0-4665-9074-00a3968da304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"device\": \"cuda\",\n",
      "  \"amp_dtype\": \"bf16\",\n",
      "  \"seed\": 42,\n",
      "  \"img_size\": 256,\n",
      "  \"batch_size\": 4,\n",
      "  \"data_root\": \"/workspace/data\",\n",
      "  \"train_dir_exists\": true,\n",
      "  \"val_dir_exists\": true,\n",
      "  \"outputs_dir\": \"/workspace/notebooks/outputs/08_adapter_lora\",\n",
      "  \"uni_repo_id\": \"hf-hub:MahmoodLab/UNI2-h\",\n",
      "  \"pixcell_repo_id\": \"StonyBrook-CVLab/PixCell-256\",\n",
      "  \"uni_out_dim\": 512,\n",
      "  \"text_ctx_dim\": 768,\n",
      "  \"tokens_from_uni\": 4,\n",
      "  \"lora_rank\": 8\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ========= Cellule 2 ‚Äî Config minimal, chemins & seed =========\n",
    "# - Fixe device, dtype, seed\n",
    "# - Centralise les chemins/projets/datasets\n",
    "# - Pr√©pare les IDs Hugging Face (√† compl√©ter selon tes repos)\n",
    "\n",
    "import os, json, random, math\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# ---- Device & dtype conseill√©s\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BF16_OK = (DEVICE == \"cuda\") and torch.cuda.is_bf16_supported()\n",
    "AMP_DTYPE = torch.bfloat16 if BF16_OK else torch.float16\n",
    "\n",
    "# ---- Seed\n",
    "SEED = 42\n",
    "def seed_everything(seed: int = 42):\n",
    "    import numpy as np, torch, random, os\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # on reste en mode perfs (pas de determinism strict)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "# ---- Arborescence projet / outputs\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "OUTPUTS_DIR  = PROJECT_ROOT / \"outputs\" / \"08_adapter_lora\"\n",
    "CHECKPOINTS_DIR = OUTPUTS_DIR / \"checkpoints\"\n",
    "SAMPLES_DIR  = OUTPUTS_DIR / \"samples\"\n",
    "for d in (OUTPUTS_DIR, CHECKPOINTS_DIR, SAMPLES_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Donn√©es locales (ajuste si besoin)\n",
    "DATA_ROOT   = Path(\"/workspace/data\")\n",
    "TRAIN_DIR   = DATA_ROOT / \"NCT-CRC-HE-100K\"\n",
    "VAL_DIR     = DATA_ROOT / \"CRC-VAL-HE-7K\"\n",
    "\n",
    "# ---- Hyperparam√®tres de base\n",
    "IMG_SIZE       = 256\n",
    "BATCH_SIZE     = 4          # commence bas (VRAM friendly)\n",
    "NUM_WORKERS    = 0          # √©viter le multiprocessing sous Jupyter+CUDA\n",
    "LEARNING_RATE  = 1e-4\n",
    "TRAIN_STEPS    = 1000\n",
    "GRAD_ACCUM     = 1\n",
    "\n",
    "# ---- Hugging Face (√† compl√©ter) : on tirera UNI2-h & PixCell directement depuis HF\n",
    "UNI_REPO_ID       = \"hf-hub:MahmoodLab/UNI2-h\"        # ex: \"acme-ai/uni2h-base\"\n",
    "UNI_REVISION      = \"main\"\n",
    "PIXCELL_REPO_ID = \"StonyBrook-CVLab/PixCell-256\"      # ex: \"acme-ai/pixcell-sd15\"\n",
    "PIXCELL_REVISION  = \"main\"\n",
    "\n",
    "# ---- Dimensions de conditionnement (adapter & cross-attn)\n",
    "UNI_OUT_DIM     = 512       # dim de l'embedding UNI2-h (√† confirmer selon le repo)\n",
    "TEXT_CTX_DIM    = 768       # dim du contexte texte (SD 1.5-like)\n",
    "TOKENS_FROM_UNI = 4         # nb de tokens inject√©s par l'adapter\n",
    "\n",
    "# ---- LoRA (cross-attn uniquement)\n",
    "LORA_RANK   = 8\n",
    "LORA_ALPHA  = 8\n",
    "LORA_SCALE  = 1.0\n",
    "LORA_TARGET = [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]  # motifs √† patcher dans le UNet\n",
    "\n",
    "# ---- Qualit√© de vie Hugging Face cache\n",
    "os.environ.setdefault(\"HF_HUB_ENABLE_HF_TRANSFER\", \"1\")      # transferts plus rapides\n",
    "os.environ.setdefault(\"HF_HOME\", str(PROJECT_ROOT / \".hf\"))  # cache local dans le projet\n",
    "\n",
    "# ---- R√©cap rapide\n",
    "summary = {\n",
    "    \"device\": DEVICE,\n",
    "    \"amp_dtype\": \"bf16\" if AMP_DTYPE is torch.bfloat16 else \"fp16\",\n",
    "    \"seed\": SEED,\n",
    "    \"img_size\": IMG_SIZE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"data_root\": str(DATA_ROOT),\n",
    "    \"train_dir_exists\": TRAIN_DIR.exists(),\n",
    "    \"val_dir_exists\": VAL_DIR.exists(),\n",
    "    \"outputs_dir\": str(OUTPUTS_DIR),\n",
    "    \"uni_repo_id\": UNI_REPO_ID,\n",
    "    \"pixcell_repo_id\": PIXCELL_REPO_ID,\n",
    "    \"uni_out_dim\": UNI_OUT_DIM,\n",
    "    \"text_ctx_dim\": TEXT_CTX_DIM,\n",
    "    \"tokens_from_uni\": TOKENS_FROM_UNI,\n",
    "    \"lora_rank\": LORA_RANK,\n",
    "}\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2e8c8-41a5-4e2e-bab4-2e76c6830dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09078077-c6d4-470b-afac-d91fef255ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'trust_remote_code': True} are not expected by PixCellPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8281038d0c42e892ea96e57afeef6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'double_self_attention': False, 'num_vector_embeds': None, 'only_cross_attention': False, 'use_linear_projection': False} were passed to PixCellTransformer2DModel, but are not expected and will be ignored. Please verify your config.json configuration file.\n",
      "The config attributes {'flow_shift': 1.0, 'use_flow_sigmas': False} were passed to DPMSolverMultistepScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline charg√©e: PixCellPipeline\n",
      "Has vae: True\n",
      "Has transformer: True\n",
      "Has scheduler: True\n",
      "Device: cuda | dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# ========= Cellule 3 ‚Äî PixCell (custom) + VAE SD3.5 LARGE ‚Äî version 07 =========\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline, AutoencoderKL\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1) VAE SD3.5 Large (subfolder=\"vae\")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-large\",\n",
    "    subfolder=\"vae\",\n",
    "    torch_dtype=torch.float16,     # comme dans 07\n",
    ")\n",
    "\n",
    "# 2) Pipeline PixCell-256 (custom pipeline + code distant)\n",
    "pipe256 = DiffusionPipeline.from_pretrained(\n",
    "    \"StonyBrook-CVLab/PixCell-256\",    # tu peux garder la variable PIXCELL_REPO_ID si tu pr√©f√®res\n",
    "    vae=vae,\n",
    "    custom_pipeline=\"StonyBrook-CVLab/PixCell-pipeline\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,         # comme dans 07\n",
    ").to(device)\n",
    "\n",
    "# 4) Sanity check minimal (pas d'acc√®s .unet ni ._modules)\n",
    "print(\"Pipeline charg√©e:\", type(pipe256).__name__)\n",
    "print(\"Has vae:\", hasattr(pipe256, \"vae\"))\n",
    "print(\"Has transformer:\", hasattr(pipe256, \"transformer\"))\n",
    "print(\"Has scheduler:\", hasattr(pipe256, \"scheduler\"))\n",
    "print(\"Device:\", device, \"| dtype:\", torch.float16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b723100-0d59-4897-b781-cbfd66294622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5142b5e8f89e4ffbad2294b50895ef92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test_image.png:   0%|          | 0.00/131k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNI emb shape: (1, 1, 1536)\n"
     ]
    }
   ],
   "source": [
    "# ========= Cellule 4 ‚Äî UNI-2h (timm) + transform =========\n",
    "# Charge UNI-2h depuis HF via timm, avec les kwargs fournis par la model card PixCell.\n",
    "# Fournit aussi une fonction utilitaire pour extraire un embedding (B, 1, D).\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import timm\n",
    "    from timm.data import resolve_data_config\n",
    "    from timm.data.transforms_factory import create_transform\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\n",
    "        \"Le package 'timm' est requis. Installe-le puis relance cette cellule :\\n\"\n",
    "        \"%pip install timm\"\n",
    "    ) from e\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Kwargs conformes √† la doc PixCell (ViT-H style)\n",
    "timm_kwargs = {\n",
    "    'img_size': 224,\n",
    "    'patch_size': 14,\n",
    "    'depth': 24,\n",
    "    'num_heads': 24,\n",
    "    'init_values': 1e-5,\n",
    "    'embed_dim': 1536,\n",
    "    'mlp_ratio': 2.66667*2,\n",
    "    'num_classes': 0,\n",
    "    'no_embed_class': True,\n",
    "    'mlp_layer': timm.layers.SwiGLUPacked,\n",
    "    'act_layer': torch.nn.SiLU,\n",
    "    'reg_tokens': 8,\n",
    "    'dynamic_img_size': True\n",
    "}\n",
    "\n",
    "# 1) Mod√®le UNI-2h pr√©-entra√Æn√© depuis HF (MahmoodLab/UNI2-h)\n",
    "uni_model = timm.create_model(\"hf-hub:MahmoodLab/UNI2-h\", pretrained=True, **timm_kwargs)\n",
    "uni_model.eval().to(device)\n",
    "\n",
    "# 2) Transform officiel d√©riv√© de la config timm\n",
    "transform = create_transform(**resolve_data_config(uni_model.pretrained_cfg, model=uni_model))\n",
    "\n",
    "# 3) Helper: encode une PIL.Image ou un batch tensor (B,3,H,W) -> (B,1,D)\n",
    "@torch.inference_mode()\n",
    "def encode_uni(input_):\n",
    "    \"\"\"\n",
    "    input_: PIL.Image ou torch.Tensor (B,3,H,W) dans [0,1]\n",
    "    retourne: torch.Tensor (B, 1, D)\n",
    "    \"\"\"\n",
    "    if isinstance(input_, torch.Tensor):\n",
    "        x = input_.to(device)\n",
    "        if x.dim() == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        # assume d√©j√† normalis√© ? sinon on passe par la transform\n",
    "        # Ici on repasse par transform par s√©curit√© : convertissons en PIL pour rester fid√®les\n",
    "        from torchvision.transforms.functional import to_pil_image\n",
    "        imgs = [transform(to_pil_image(t.cpu().clamp(0,1))) for t in x]\n",
    "        x = torch.stack(imgs, dim=0).to(device)\n",
    "    else:\n",
    "        # PIL.Image -> tensor via transform\n",
    "        x = transform(input_).unsqueeze(0).to(device)\n",
    "\n",
    "    z = uni_model(x)          # (B, D)\n",
    "    z = z.unsqueeze(1)        # (B, 1, D)\n",
    "    return z\n",
    "\n",
    "# 4) Mini smoke test avec l'image d'exemple du repo PixCell\n",
    "from huggingface_hub import hf_hub_download\n",
    "from PIL import Image\n",
    "\n",
    "test_path = hf_hub_download(repo_id=\"StonyBrook-CVLab/PixCell-256\", filename=\"test_image.png\")\n",
    "img = Image.open(test_path).convert(\"RGB\")\n",
    "z_uni = encode_uni(img)\n",
    "print(\"UNI emb shape:\", tuple(z_uni.shape))  # attendu: (1, 1, 1536)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623e2b2f-b632-48f9-8f85-f358cef1f98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
