{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np, random, os\n",
    "import torch\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Seed={SEED} | Device={DEVICE}\")\n",
    "torch.cuda.empty_cache() if DEVICE==\"cuda\" else None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT  = Path(os.getenv(\"PROJECT_ROOT\", \"/workspace\")).resolve()\n",
    "DATA_ROOT     = Path(os.getenv(\"DATA_ROOT\", \"/workspace/data\")).resolve()\n",
    "OUTPUTS_DIR   = PROJECT_ROOT / \"outputs\"\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / \"artifacts\"\n",
    "CONFIG_DIR    = PROJECT_ROOT / \"configs\"\n",
    "SAMPLES_DIR   = PROJECT_ROOT / \"samples\"\n",
    "CKPT_DIR      = PROJECT_ROOT / \"checkpoints\"\n",
    "METRICS_DIR   = PROJECT_ROOT / \"metrics/gan_metrics\"\n",
    "for d in [OUTPUTS_DIR, ARTIFACTS_DIR, CONFIG_DIR, SAMPLES_DIR, CKPT_DIR, METRICS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = (DEVICE==\"cuda\")\n",
    "DROP_LAST = True\n",
    "SHUFFLE = True\n",
    "\n",
    "# Classes ciblées (utilise le filtrage du HistoDataset)\n",
    "SELECTED_CLASSES = [\"TUM\", \"STR\", \"NORM\"]  # mets None pour toutes\n",
    "print(\"CONFIG_DIR:\", CONFIG_DIR)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from p9dg.histo_dataset import HistoDataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "PIXEL_RANGE = \"-1_1\"\n",
    "SAMPLES_PER_CLASS = 300\n",
    "VAHADANE_ENABLE = True\n",
    "\n",
    "ds_gan = HistoDataset(\n",
    "    root_data=str(DATA_ROOT),\n",
    "    split=\"train\",\n",
    "    output_size=IMAGE_SIZE,\n",
    "    pixel_range=PIXEL_RANGE,\n",
    "    balance_per_class=True,\n",
    "    samples_per_class_per_epoch=SAMPLES_PER_CLASS,\n",
    "    vahadane_enable=VAHADANE_ENABLE,\n",
    "    vahadane_device=DEVICE,\n",
    "    thresholds_json_path=str(CONFIG_DIR / \"seuils_par_classe.json\"),\n",
    "    return_labels=True,\n",
    "    classes=SELECTED_CLASSES,\n",
    ")\n",
    "print(\"✅ Dataset cGAN initialisé\")\n",
    "print(\"Classes retenues:\", ds_gan.class_counts())\n",
    "num_classes = len(ds_gan.class_counts())\n",
    "print(\"num_classes =\", num_classes)\n",
    "\n",
    "loader_gan = DataLoader(\n",
    "    dataset=ds_gan,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    "    drop_last=DROP_LAST,\n",
    ")\n",
    "xb, yb, _ = next(iter(loader_gan))\n",
    "print(f\"Batch: x={tuple(xb.shape)}, y={tuple(yb.shape)} → min={xb.min():.3f}, max={xb.max():.3f}\")\n",
    "\n",
    "# held-out pour métriques\n",
    "import numpy as np\n",
    "HELDOUT_IDX_PATH = METRICS_DIR / \"duet_eval_idx.npy\"\n",
    "N_EVAL_POOL = 2000\n",
    "if not HELDOUT_IDX_PATH.exists():\n",
    "    rng = np.random.default_rng(42)\n",
    "    all_idx = np.arange(len(ds_gan))\n",
    "    pick = min(N_EVAL_POOL, len(all_idx))\n",
    "    eval_idx = rng.choice(all_idx, size=pick, replace=False)\n",
    "    np.save(HELDOUT_IDX_PATH, eval_idx)\n",
    "else:\n",
    "    eval_idx = np.load(HELDOUT_IDX_PATH)\n",
    "\n",
    "class StripXY(torch.utils.data.Dataset):\n",
    "    def __init__(self, base): self.base=base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i): x, y, p = self.base[i]; return x\n",
    "\n",
    "ds_eval = Subset(ds_gan, eval_idx.tolist())\n",
    "loader_real_eval = DataLoader(StripXY(ds_eval), batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "print(\"✅ loader_real_eval prêt:\", len(ds_eval))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# PathoDuet extractor (gelé) basé sur timm (DeiT-B/16 224), chargement ckpt HE\n",
    "import re, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "try:\n",
    "    import timm\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Installe 'timm' (pip install timm)\")\n",
    "\n",
    "CKPT_PATH = PROJECT_ROOT / \"models/checkpoint_HE.pth\"\n",
    "assert CKPT_PATH.exists(), f\"Checkpoint introuvable: {CKPT_PATH}\"\n",
    "\n",
    "def load_state_dict_safely(path):\n",
    "    sd_raw = None\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "        if isinstance(obj, dict) and all(hasattr(v, \"shape\") for v in obj.values()):\n",
    "            sd_raw = obj\n",
    "        elif isinstance(obj, dict):\n",
    "            for k in (\"state_dict\",\"model\",\"module\"):\n",
    "                if k in obj and isinstance(obj[k], dict) and all(hasattr(v, \"shape\") for v in obj[k].values()):\n",
    "                    sd_raw = obj[k]; break\n",
    "    except TypeError:\n",
    "        obj = torch.load(path, map_location=\"cpu\")\n",
    "        if isinstance(obj, dict) and all(hasattr(v, \"shape\") for v in obj.values()):\n",
    "            sd_raw = obj\n",
    "        elif isinstance(obj, dict):\n",
    "            for k in (\"state_dict\",\"model\",\"module\"):\n",
    "                if k in obj and isinstance(obj[k], dict) and all(hasattr(v, \"shape\") for v in obj[k].values()):\n",
    "                    sd_raw = obj[k]; break\n",
    "    if sd_raw is None:\n",
    "        raise RuntimeError(\"state_dict non trouvé dans le ckpt\")\n",
    "    clean_sd = {re.sub(r\"^(module\\.|model\\.)\",\"\",k): v for k, v in sd_raw.items()}\n",
    "    # retirer distillation + réajuster pos_embed 198→197\n",
    "    clean_sd.pop(\"dist_token\", None)\n",
    "    for k in list(clean_sd.keys()):\n",
    "        if k.startswith(\"head_dist.\"): clean_sd.pop(k)\n",
    "    if \"pos_embed\" in clean_sd and clean_sd[\"pos_embed\"].shape[1] == 198:\n",
    "        pos = clean_sd[\"pos_embed\"]\n",
    "        clean_sd[\"pos_embed\"] = torch.cat([pos[:, :1, :], pos[:, 2:, :]], dim=1)\n",
    "    for k in list(clean_sd.keys()):\n",
    "        if k.startswith(\"head.\"): clean_sd.pop(k)\n",
    "    return clean_sd\n",
    "\n",
    "state_dict = load_state_dict_safely(CKPT_PATH)\n",
    "backbone = timm.create_model(\"deit_base_patch16_224\", pretrained=False, num_classes=0, img_size=224, global_pool=\"avg\")\n",
    "missing, unexpected = backbone.load_state_dict(state_dict, strict=False)\n",
    "for p in backbone.parameters(): p.requires_grad_(False)\n",
    "backbone.eval()\n",
    "\n",
    "class PathoDuetExtractor(nn.Module):\n",
    "    def __init__(self, bb, in_size=224):\n",
    "        super().__init__(); self.bb=bb; self.in_size=in_size\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x_m11):\n",
    "        x01 = (x_m11.clamp(-1,1) + 1)*0.5\n",
    "        if x01.shape[-2:] != (self.in_size, self.in_size):\n",
    "            x01 = F.interpolate(x01, size=(self.in_size,self.in_size), mode=\"bilinear\", align_corners=False)\n",
    "        return self.bb(x01)\n",
    "\n",
    "pathoduet = PathoDuetExtractor(backbone).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    f = pathoduet(xb.to(DEVICE)[:2])\n",
    "print(\"PathoDuet features:\", tuple(f.shape))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# cGAN: Générateur (StyleGAN-lite) conditionnel (embedding classe → mapping)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512, n_layers=8):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dim = z_dim\n",
    "        for _ in range(n_layers):\n",
    "            layers += [nn.Linear(dim, w_dim), nn.LeakyReLU(0.2, inplace=True)]\n",
    "            dim = w_dim\n",
    "        self.mapping = nn.Sequential(*layers)\n",
    "    def forward(self, z):\n",
    "        z = z / (z.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        return self.mapping(z)\n",
    "\n",
    "class ModulatedConv2d(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel, style_dim, demod=True, up=False):\n",
    "        super().__init__(); self.up=up; self.demod=demod; self.eps=1e-8; self.pad=kernel//2\n",
    "        self.weight = nn.Parameter(torch.randn(1, out_ch, in_ch, kernel, kernel))\n",
    "        self.style = nn.Linear(style_dim, in_ch)\n",
    "    def forward(self, x, s):\n",
    "        if self.up:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        b, c, h, w = x.shape\n",
    "        w1 = self.style(s).view(b, 1, c, 1, 1)\n",
    "        w2 = self.weight * (w1 + 1)\n",
    "        if self.demod:\n",
    "            d = torch.rsqrt((w2 ** 2).sum([2,3,4]) + self.eps)\n",
    "            w2 = w2 * d.view(b, -1, 1, 1, 1)\n",
    "        x = x.view(1, -1, h, w)\n",
    "        w2 = w2.view(b * w2.size(1), w2.size(2), w2.size(3), w2.size(4))\n",
    "        out = F.conv2d(x, w2, padding=self.pad, groups=b)\n",
    "        return out.view(b, -1, out.shape[-2], out.shape[-1])\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=512, w_dim=512, img_res=256, fmap_base=256, num_classes=9):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.embed = nn.Embedding(num_classes, z_dim)\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "        self.mapping = MappingNetwork(z_dim, w_dim)\n",
    "        self.const = nn.Parameter(torch.randn(1, fmap_base, 4, 4))\n",
    "        self.blocks = nn.ModuleList()\n",
    "        in_ch = fmap_base; res = 4\n",
    "        while res < img_res:\n",
    "            out_ch = max(fmap_base // max(res // 8, 1), 64)\n",
    "            self.blocks.append(nn.ModuleList([\n",
    "                ModulatedConv2d(in_ch, out_ch, 3, w_dim, up=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "                ModulatedConv2d(out_ch, out_ch, 3, w_dim, up=False), nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]))\n",
    "            in_ch = out_ch; res *= 2\n",
    "        self.to_rgb = nn.Conv2d(in_ch, 3, 1)\n",
    "    def forward(self, z, y):\n",
    "        zc = z + self.embed(y)\n",
    "        w = self.mapping(zc)\n",
    "        x = self.const.repeat(z.size(0), 1, 1, 1)\n",
    "        for (m1,a1,m2,a2) in self.blocks:\n",
    "            x = m1(x, w); x = a1(x); x = m2(x, w); x = a2(x)\n",
    "        return torch.tanh(self.to_rgb(x))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ch=64):\n",
    "        super().__init__()\n",
    "        def C(i,o,k=3,s=2,p=1): return nn_utils.spectral_norm(nn.Conv2d(i,o,k,s,p))\n",
    "        self.body = nn.Sequential(\n",
    "            C(3, ch,3,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            C(ch, ch*2,3,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            C(ch*2, ch*4,3,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            C(ch*4, ch*8,3,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            C(ch*8, ch*8,3,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            C(ch*8, ch*8,3,2,1), nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.conv_last = nn_utils.spectral_norm(nn.Conv2d(ch*8, ch*8, 4, 1, 0))\n",
    "        self.to_logit  = nn_utils.spectral_norm(nn.Conv2d(ch*8, 1, 1, 1, 0))\n",
    "        self.out_ch = ch*8\n",
    "    def forward(self, x):\n",
    "        h = self.body(x)\n",
    "        h = self.conv_last(h)\n",
    "        logit_base = self.to_logit(h).view(x.size(0), 1)\n",
    "        feat = h.view(x.size(0), self.out_ch)\n",
    "        return logit_base, feat\n",
    "\n",
    "class CombinedCondDiscriminator(nn.Module):\n",
    "    def __init__(self, base, num_classes, duet_extractor, alpha=0.0, t_duet=2.0):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.embed = nn.Embedding(num_classes, base.out_ch)\n",
    "        nn.init.normal_(self.embed.weight, std=0.02)\n",
    "        self.duet = duet_extractor\n",
    "        self.duet_head = nn.Linear(768, 1)\n",
    "        self.alpha = float(alpha); self.t_duet=float(t_duet)\n",
    "    def forward(self, x_m11, y):\n",
    "        logit_base, feat = self.base(x_m11)\n",
    "        wy = self.embed(y)\n",
    "        logit_proj = (feat * wy).sum(dim=1, keepdim=True)\n",
    "        with torch.no_grad():\n",
    "            f_duet = self.duet(x_m11)\n",
    "        logit_duet = self.duet_head(f_duet)\n",
    "        if self.t_duet>1.0:\n",
    "            logit_duet = logit_duet / self.t_duet\n",
    "        return logit_base + logit_proj + self.alpha * logit_duet\n",
    "\n",
    "Z_DIM = 512\n",
    "G = Generator(z_dim=Z_DIM, w_dim=Z_DIM, img_res=IMAGE_SIZE, fmap_base=256, num_classes=num_classes).to(DEVICE)\n",
    "D_base = Discriminator(ch=64).to(DEVICE)\n",
    "D = CombinedCondDiscriminator(D_base, num_classes=num_classes, duet_extractor=pathoduet, alpha=0.0, t_duet=2.0).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(4, Z_DIM, device=DEVICE)\n",
    "    y = torch.randint(0, num_classes, (4,), device=DEVICE)\n",
    "    fake = G(z, y)\n",
    "    out = D(fake, y)\n",
    "print(\"Smoke: fake=\", tuple(fake.shape), \"D(out)=\", tuple(out.shape))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Entraînement NS-GAN + R1 + EMA + ADA minimal, FID PathoDuet conditionnel\n",
    "import time\n",
    "import torchvision.utils as vutils\n",
    "from csv import DictWriter\n",
    "from contextlib import nullcontext\n",
    "\n",
    "def requires_grad(m, flag=True):\n",
    "    for p in m.parameters(): p.requires_grad_(flag)\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(ema_m, m, decay=0.9995):\n",
    "    for p_ema, p in zip(ema_m.parameters(), m.parameters()):\n",
    "        p_ema.data.mul_(decay).add_(p.data, alpha=1.0-decay)\n",
    "\n",
    "def d_logistic_loss(r, f):\n",
    "    return torch.nn.functional.softplus(-r).mean() + torch.nn.functional.softplus(f).mean()\n",
    "def g_nonsat_loss(f):\n",
    "    return torch.nn.functional.softplus(-f).mean()\n",
    "def r1_penalty(x, pred):\n",
    "    grad = torch.autograd.grad(outputs=pred.sum(), inputs=x, create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "    return grad.pow(2).flatten(1).sum(1).mean()\n",
    "\n",
    "def ada_augment(x, p, translate=0.04):\n",
    "    if p<=0: return x\n",
    "    if torch.rand(1, device=x.device).item() < p:\n",
    "        x = torch.flip(x, dims=[3])\n",
    "    if torch.rand(1, device=x.device).item() < p:\n",
    "        h, w = x.shape[-2:]\n",
    "        max_pix = max(1, int(h*translate))\n",
    "        dx = torch.randint(-max_pix, max_pix+1, (1,), device=x.device).item()\n",
    "        dy = torch.randint(-max_pix, max_pix+1, (1,), device=x.device).item()\n",
    "        x = torch.roll(x, shifts=(dx,dy), dims=(2,3))\n",
    "    return x\n",
    "\n",
    "ADA_STATE = {\"p\": 0.0, \"acc_ema\": None}\n",
    "ADA_TARGET=0.6; ADA_DECAY=0.99; ADA_SPEED=0.25; ADA_MAX_P=0.08\n",
    "R1_GAMMA=10.0; R1_EVERY=8\n",
    "EPOCHS=2; LR_G=4e-4; LR_D=1e-4; BETAS=(0.0,0.99)\n",
    "SAMPLE_EVERY=400; SAVE_EVERY=800\n",
    "\n",
    "opt_G = torch.optim.Adam(G.parameters(), lr=LR_G, betas=BETAS)\n",
    "opt_D = torch.optim.Adam(D.parameters(), lr=LR_D, betas=BETAS)\n",
    "\n",
    "try:\n",
    "    from torch.amp import GradScaler as _GradScaler\n",
    "    from torch.cuda.amp import autocast as autocast_cm\n",
    "except Exception:\n",
    "    from torch.cuda.amp import GradScaler as _GradScaler\n",
    "    def autocast_cm(enabled=True): return nullcontext()\n",
    "USE_AMP = False\n",
    "scaler_G=_GradScaler(enabled=USE_AMP); scaler_D=_GradScaler(enabled=USE_AMP)\n",
    "\n",
    "class RealStatsEMA:\n",
    "    def __init__(self, m=0.995): self.m=m; self.mean=None; self.std=None\n",
    "    @torch.no_grad()\n",
    "    def update(self, x):\n",
    "        x01=(x.clamp(-1,1)+1)*0.5\n",
    "        mean=x01.mean(dim=(0,2,3)); std=x01.std(dim=(0,2,3)).clamp_min(1e-6)\n",
    "        if self.mean is None: self.mean, self.std = mean, std\n",
    "        else:\n",
    "            self.mean = self.m*self.mean + (1-self.m)*mean\n",
    "            self.std  = self.m*self.std  + (1-self.m)*std\n",
    "    def penalty(self, f):\n",
    "        if self.mean is None: return f.new_zeros(())\n",
    "        f01=(f.clamp(-1,1)+1)*0.5\n",
    "        fmean=f01.mean(dim=(0,2,3)); fstd=f01.std(dim=(0,2,3)).clamp_min(1e-6)\n",
    "        return (fmean-self.mean.detach()).pow(2).mean() + (fstd-self.std.detach()).pow(2).mean()\n",
    "\n",
    "real_stats = RealStatsEMA(0.995); LAMBDA_STATS = 3e-3\n",
    "\n",
    "@torch.no_grad()\n",
    "def duet_frechet_cond(real_batch_iter, G_ema, z_dim, num_classes, n_real=128, n_fake=128, device=None):\n",
    "    dev = device or next(G_ema.parameters()).device\n",
    "    def _resize224(x):\n",
    "        x01 = (x.clamp(-1,1)+1)*0.5\n",
    "        return torch.nn.functional.interpolate(x01, size=(224,224), mode=\"bilinear\", align_corners=False)\n",
    "    # real\n",
    "    feats_r, n_acc = [], 0\n",
    "    for x in real_batch_iter:\n",
    "        x = x.to(dev)\n",
    "        x224 = _resize224(x)\n",
    "        f = pathoduet(x224)\n",
    "        feats_r.append(f)\n",
    "        n_acc += x.size(0)\n",
    "        if n_acc >= n_real: break\n",
    "    feats_r = torch.cat(feats_r, dim=0)[:n_real]\n",
    "    # fake\n",
    "    z = torch.randn(n_fake, z_dim, device=dev)\n",
    "    y = torch.randint(0, num_classes, (n_fake,), device=dev)\n",
    "    fake = G_ema(z, y)\n",
    "    f224 = _resize224(fake)\n",
    "    feats_f = pathoduet(f224)\n",
    "    # moments\n",
    "    def _mom(fe):\n",
    "        mu = fe.mean(dim=0); xc=fe-mu; cov = (xc.t()@xc)/(fe.size(0)-1+1e-8)\n",
    "        return mu, cov\n",
    "    mu_r, cov_r = _mom(feats_r); mu_f, cov_f = _mom(feats_f)\n",
    "    def _fre(mu1,c1,mu2,c2):\n",
    "        m1=mu1.double(); m2=mu2.double(); C1=c1.double(); C2=c2.double()\n",
    "        diff=(m1-m2)\n",
    "        eva1,eve1=torch.linalg.eigh(C1+1e-6*torch.eye(C1.shape[0], device=dev));\n",
    "        eva2,eve2=torch.linalg.eigh(C2+1e-6*torch.eye(C2.shape[0], device=dev));\n",
    "        C1h=(eve1@torch.diag_embed(eva1.clamp_min(0).sqrt())@eve1.t())\n",
    "        C2h=(eve2@torch.diag_embed(eva2.clamp_min(0).sqrt())@eve2.t())\n",
    "        evaP,eveP=torch.linalg.eigh((C1h@C2h)@ (C1h@C2h))\n",
    "        sqrtP=(eveP@torch.diag_embed(evaP.clamp_min(0).sqrt())@eveP.t())\n",
    "        return (diff@diff).item() + torch.trace(C1+C2-2*sqrtP).item()\n",
    "    return float(_fre(mu_r,cov_r,mu_f,cov_f))\n",
    "\n",
    "def log_metrics_csv(path, row):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    is_new = (not path.exists()) or path.stat().st_size==0\n",
    "    with path.open(\"a\", newline=\"\") as f:\n",
    "        w = DictWriter(f, fieldnames=[\"epoch\",\"step\",\"d_loss\",\"g_loss\",\"real_mu\",\"real_min\",\"real_max\",\"fake_mu\",\"fake_min\",\"fake_max\",\"ada_p\",\"acc\",\"acc_ema\",\"alpha\"])\n",
    "        if is_new: w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "G_ema = Generator(z_dim=Z_DIM, w_dim=Z_DIM, img_res=IMAGE_SIZE, fmap_base=G.const.shape[1], num_classes=num_classes).to(DEVICE)\n",
    "G_ema.load_state_dict(G.state_dict()); requires_grad(G_ema, False)\n",
    "\n",
    "global_step=0\n",
    "print(\"[init] D.alpha=0.0 (branche PathoDuet)\")\n",
    "D.alpha = 0.0\n",
    "METRICS_CSV = OUTPUTS_DIR / \"gan/metrics_gan.csv\"\n",
    "if METRICS_CSV.exists(): METRICS_CSV.unlink()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t0=time.time()\n",
    "    for real, y_real, _ in loader_gan:\n",
    "        global_step += 1\n",
    "        real = real.to(DEVICE, non_blocking=True)\n",
    "        y_real = y_real.to(DEVICE, non_blocking=True)\n",
    "        # D\n",
    "        for p in D.parameters(): p.requires_grad_(True)\n",
    "        for p in G.parameters(): p.requires_grad_(False)\n",
    "        y_fake = torch.randint(0, num_classes, (real.size(0),), device=DEVICE)\n",
    "        z = torch.randn(real.size(0), Z_DIM, device=DEVICE)\n",
    "        opt_D.zero_grad(set_to_none=True)\n",
    "        with autocast_cm(enabled=USE_AMP):\n",
    "            fake = G(z, y_fake).detach()\n",
    "            real_aug = ada_augment(real, ADA_STATE[\"p\"], translate=0.02)\n",
    "            fake_aug = ada_augment(fake, ADA_STATE[\"p\"], translate=0.02)\n",
    "            real_pred = D(real_aug, y_real)\n",
    "            fake_pred = D(fake_aug, y_fake)\n",
    "            d_loss = d_logistic_loss(real_pred, fake_pred)\n",
    "        scaler_D.scale(d_loss).backward()\n",
    "        do_r1 = (global_step % R1_EVERY)==0\n",
    "        if do_r1:\n",
    "            real_r1 = real.detach().requires_grad_(True)\n",
    "            with autocast_cm(enabled=USE_AMP):\n",
    "                real_aug_r1 = ada_augment(real_r1, ADA_STATE[\"p\"], translate=0.04)\n",
    "                real_pred_r1 = D(real_aug_r1, y_real)\n",
    "            r1 = 0.5*R1_GAMMA*r1_penalty(real_aug_r1, real_pred_r1)\n",
    "            scaler_D.scale(r1).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(D.parameters(), 1.0)\n",
    "        scaler_D.step(opt_D); scaler_D.update()\n",
    "        with torch.no_grad(): real_stats.update(real)\n",
    "        # ADA simple (EMA accuracy)\n",
    "        with torch.no_grad():\n",
    "            acc = torch.sigmoid(real_pred).gt(0.5).float().mean().item()\n",
    "            ADA_STATE[\"acc_ema\"] = acc if ADA_STATE[\"acc_ema\"] is None else ADA_DECAY*ADA_STATE[\"acc_ema\"] + (1-ADA_DECAY)*acc\n",
    "            err = ADA_STATE[\"acc_ema\"] - ADA_TARGET\n",
    "            ADA_STATE[\"p\"] = float(min(ADA_MAX_P, max(0.0, ADA_STATE[\"p\"] + 0.001*err*ADA_SPEED)))\n",
    "\n",
    "        # G\n",
    "        for p in D.parameters(): p.requires_grad_(False)\n",
    "        for p in G.parameters(): p.requires_grad_(True)\n",
    "        y_fake = torch.randint(0, num_classes, (real.size(0),), device=DEVICE)\n",
    "        z = torch.randn(real.size(0), Z_DIM, device=DEVICE)\n",
    "        opt_G.zero_grad(set_to_none=True)\n",
    "        with autocast_cm(enabled=USE_AMP):\n",
    "            fake = G(z, y_fake)\n",
    "            fake_aug = ada_augment(fake, ADA_STATE[\"p\"], translate=0.03)\n",
    "            fake_pred = D(fake_aug, y_fake)\n",
    "            g_loss = g_nonsat_loss(fake_pred) + LAMBDA_STATS*real_stats.penalty(fake)\n",
    "        scaler_G.scale(g_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(G.parameters(), 1.0)\n",
    "        scaler_G.step(opt_G); scaler_G.update()\n",
    "        ema_update(G_ema, G, decay=0.9995)\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                r_mean, r_min, r_max = real.mean().item(), real.min().item(), real.max().item()\n",
    "                f_clamp = fake.clamp(-1,1)\n",
    "                f_mean, f_min, f_max = f_clamp.mean().item(), f_clamp.min().item(), f_clamp.max().item()\n",
    "                alpha = getattr(D, 'alpha', 0.0)\n",
    "            log_metrics_csv(OUTPUTS_DIR/\"gan/metrics_gan.csv\", {\n",
    "                \"epoch\": epoch+1, \"step\": global_step,\n",
    "                \"d_loss\": float(d_loss.item()), \"g_loss\": float(g_loss.item()),\n",
    "                \"real_mu\": r_mean, \"real_min\": r_min, \"real_max\": r_max,\n",
    "                \"fake_mu\": f_mean, \"fake_min\": f_min, \"fake_max\": f_max,\n",
    "                \"ada_p\": ADA_STATE[\"p\"], \"acc\": acc, \"acc_ema\": ADA_STATE[\"acc_ema\"], \"alpha\": alpha\n",
    "            })\n",
    "\n",
    "        if global_step % SAMPLE_EVERY == 0:\n",
    "            with torch.no_grad():\n",
    "                z_vis = torch.randn(16, Z_DIM, device=DEVICE)\n",
    "                y_vis = torch.randint(0, num_classes, (16,), device=DEVICE)\n",
    "                imgs = G_ema(z_vis, y_vis).clamp(-1,1)\n",
    "                vutils.save_image((imgs+1)*0.5, str(SAMPLES_DIR / f\"sample_step{global_step:06d}.png\"), nrow=4)\n",
    "                try:\n",
    "                    fd = duet_frechet_cond(loader_real_eval, G_ema, Z_DIM, num_classes, n_real=96, n_fake=96, device=DEVICE)\n",
    "                    print(f\"[Duet-FID] step {global_step:06d} -> {fd:.3f}\")\n",
    "                except Exception as e:\n",
    "                    print(\"[Duet-FID] skip:\", e)\n",
    "\n",
    "        if global_step % SAVE_EVERY == 0:\n",
    "            torch.save({\n",
    "                \"G\": G.state_dict(), \"D\": D.state_dict(), \"G_ema\": G_ema.state_dict(),\n",
    "                \"opt_G\": opt_G.state_dict(), \"opt_D\": opt_D.state_dict(),\n",
    "                \"step\": global_step, \"epoch\": epoch, \"num_classes\": num_classes,\n",
    "            }, CKPT_DIR / f\"cgan_step{global_step:06d}.pt\")\n",
    "\n",
    "    print(f\"✅ Epoch {epoch+1}/{EPOCHS} OK ({(time.time()-t0)/60:.1f} min)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
