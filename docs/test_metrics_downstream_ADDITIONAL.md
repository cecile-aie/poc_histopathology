# Tests suppl√©mentaires pour `cnn_eval.py`

## üìä Analyse du notebook existant

Le notebook `test_metrics_downstream.ipynb` teste d√©j√† :
- ‚úÖ Import des modules
- ‚úÖ √âvaluation depuis CSV avec calibration
- ‚úÖ Inf√©rence compl√®te sur images real/synth
- ‚úÖ PCR (Pair Consistency Rate)
- ‚úÖ M√©triques compl√®tes (ECE, Brier, Accuracy, F1)
- ‚úÖ Visualisation d'erreurs

## üß™ Tests suppl√©mentaires recommand√©s

### 1. Tests unitaires des fonctions individuelles

#### Test `compute_ece()`
```python
# Test avec probabilit√©s parfaitement calibr√©es (ECE = 0)
# Test avec probabilit√©s non calibr√©es (ECE > 0)
# Test avec diff√©rents nombres de bins
# Test avec un seul √©chantillon
# Test avec toutes les pr√©dictions correctes
```

#### Test `fit_temperature()`
```python
# Test que T=1.0 pour un mod√®le d√©j√† calibr√©
# Test que T am√©liore l'ECE
# Test avec diff√©rents nombres d'it√©rations
# Test avec logits tr√®s confiants vs peu confiants
```

#### Test `load_temperature()` et `fit_temperature_from_csv()`
```python
# Test chargement temp√©rature depuis JSON valide
# Test chargement temp√©rature depuis JSON invalide
# Test chargement temp√©rature depuis fichier inexistant
# Test fit depuis CSV avec logits valides
# Test fit depuis CSV avec logits invalides
```

#### Test `_resolve_classes()`
```python
# Test avec class_to_idx fourni
# Test sans class_to_idx (fallback g√©n√©rique)
# Test avec class_mappings disponible
# Test avec class_mappings indisponible
```

### 2. Tests de cas limites et robustesse

#### CSV invalides ou incomplets
```python
# CSV vide
# CSV sans colonne 'logits_json'
# CSV sans colonne 'y_true'
# CSV avec logits_json invalide (pas JSON)
# CSV avec logits de longueurs diff√©rentes
# CSV avec y_true hors bornes (n√©gatif ou >= n_classes)
```

#### Donn√©es invalides
```python
# Logits avec NaN
# Logits avec Inf
# Probabilit√©s qui ne somment pas √† 1
# Labels n√©gatifs
# Labels >= n_classes
```

#### Temp√©rature invalide
```python
# Temp√©rature = 0
# Temp√©rature n√©gative
# Temp√©rature = Inf
# Temp√©rature = NaN
# JSON avec format invalide
```

### 3. Tests de coh√©rence

#### Coh√©rence des m√©triques
```python
# V√©rifier que ECE calibr√© <= ECE raw (normalement)
# V√©rifier que Brier calibr√© <= Brier raw (normalement)
# V√©rifier que accuracy = (y_pred == y_true).mean()
# V√©rifier que les probabilit√©s somment √† 1
# V√©rifier que top1_conf correspond √† max(probs)
```

#### Coh√©rence entre fonctions
```python
# V√©rifier que export_predictions() produit un CSV valide pour evaluate_csv_metrics()
# V√©rifier que fit_temperature_from_csv() produit le m√™me T que fit_temperature()
# V√©rifier que run_eval_split() produit les m√™mes r√©sultats que le pipeline manuel
```

### 4. Tests de r√©utilisation

#### R√©utilisation des fichiers
```python
# Charger temp√©rature depuis JSON et l'appliquer √† un nouveau CSV
# √âvaluer le m√™me CSV plusieurs fois (doit donner les m√™mes r√©sultats)
# √âvaluer un CSV avec diff√©rentes temp√©ratures
# √âvaluer un CSV sans temp√©rature puis avec temp√©rature
```

### 5. Tests de performance

#### Performance des fonctions
```python
# Mesurer le temps d'ex√©cution de compute_ece() sur diff√©rents tailles
# Mesurer le temps d'ex√©cution de fit_temperature() sur diff√©rents tailles
# Mesurer le temps d'ex√©cution de export_predictions() sur diff√©rents batch sizes
# V√©rifier que l'AMP acc√©l√®re l'inf√©rence
```

### 6. Tests d'int√©gration

#### Pipeline complet
```python
# Test run_eval_split() avec fit_temperature_on_val=True
# Test run_eval_split() avec temp_json fourni
# Test run_eval_split() sans calibration
# Test run_eval_split() avec make_plots=False
# Test run_eval_split() avec normalize_cm=True
```

### 7. Tests de visualisation

#### G√©n√©ration des figures
```python
# V√©rifier que les figures sont cr√©√©es quand plot=True
# V√©rifier que les figures ne sont pas cr√©√©es quand plot=False
# V√©rifier que les chemins des figures sont corrects
# V√©rifier que les figures sont lisibles (pas vides)
```

## üìù Exemple de cellules √† ajouter au notebook

### Cellule : Tests unitaires ECE
```python
# Test compute_ece avec probabilit√©s parfaitement calibr√©es
import torch
from metrics.cnn_eval import compute_ece

# Cas id√©al : confiance = accuracy pour chaque bin
n_samples = 1000
n_classes = 9
y_true = torch.randint(0, n_classes, (n_samples,))
# Cr√©er des probabilit√©s parfaitement calibr√©es
probs = torch.rand(n_samples, n_classes)
probs = probs / probs.sum(dim=1, keepdim=True)
# Ajuster pour que la confiance max = accuracy
for i in range(n_samples):
    pred = probs[i].argmax()
    if pred == y_true[i]:
        probs[i, pred] = 0.9  # Haute confiance pour pr√©diction correcte
    else:
        probs[i, pred] = 0.1  # Basse confiance pour pr√©diction incorrecte
    probs[i] = probs[i] / probs[i].sum()

ece = compute_ece(probs, y_true)
print(f"ECE pour probabilit√©s calibr√©es : {ece:.6f} (devrait √™tre proche de 0)")
assert ece < 0.1, "ECE trop √©lev√© pour probabilit√©s calibr√©es"
```

### Cellule : Tests de robustesse CSV
```python
# Test avec CSV invalide
import pandas as pd
import json
from pathlib import Path
from metrics.cnn_eval import evaluate_csv_metrics

# Cr√©er un CSV de test avec donn√©es invalides
test_csv = Path("/tmp/test_invalid.csv")
df_test = pd.DataFrame({
    "split": ["val"] * 10,
    "image_path": [f"test_{i}.png" for i in range(10)],
    "y_true": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],  # 9 est hors bornes si n_classes=9
    "y_pred": [0] * 10,
    "top1_conf": [0.9] * 10,
    "logits_json": [json.dumps([1.0] * 8) for _ in range(10)]  # 8 classes au lieu de 9
})
df_test.to_csv(test_csv, index=False)

# Tester que la fonction g√®re les erreurs gracieusement
try:
    report = evaluate_csv_metrics(test_csv, plot=False)
    print("‚ö†Ô∏è La fonction n'a pas d√©tect√© l'erreur")
except Exception as e:
    print(f"‚úÖ Erreur d√©tect√©e correctement : {type(e).__name__}")
```

### Cellule : Test de coh√©rence temp√©rature
```python
# V√©rifier que fit_temperature() et fit_temperature_from_csv() donnent le m√™me r√©sultat
import torch
import pandas as pd
import json
from pathlib import Path
from metrics.cnn_eval import fit_temperature, fit_temperature_from_csv, export_predictions

# Cr√©er des donn√©es de test
n_samples = 100
n_classes = 9
logits = torch.randn(n_samples, n_classes)
y_true = torch.randint(0, n_classes, (n_samples,))

# Fit temp√©rature directement
scaler1 = fit_temperature(logits, y_true)
T1 = float(torch.exp(scaler1.log_t).item())

# Export vers CSV puis fit depuis CSV
test_csv = Path("/tmp/test_temp.csv")
df = pd.DataFrame({
    "split": ["val"] * n_samples,
    "image_path": [f"test_{i}.png" for i in range(n_samples)],
    "y_true": y_true.tolist(),
    "y_pred": [0] * n_samples,
    "top1_conf": [0.9] * n_samples,
    "logits_json": [json.dumps(logits[i].tolist()) for i in range(n_samples)]
})
df.to_csv(test_csv, index=False)

test_json = Path("/tmp/test_temp.json")
T2 = fit_temperature_from_csv(test_csv, test_json)

# Comparer
diff = abs(T1 - T2)
print(f"T1 (direct) = {T1:.6f}")
print(f"T2 (CSV)    = {T2:.6f}")
print(f"Diff√©rence  = {diff:.6f}")
assert diff < 1e-3, f"Temp√©ratures diff√©rentes : {diff}"
print("‚úÖ Les deux m√©thodes donnent le m√™me r√©sultat")
```

### Cellule : Test de r√©utilisation
```python
# V√©rifier qu'on peut r√©utiliser un CSV avec diff√©rentes temp√©ratures
from metrics.cnn_eval import evaluate_csv_metrics, load_temperature
import json
from pathlib import Path

# Charger le CSV de r√©f√©rence
val_csv = Path("/workspace/outputs/baseline/mobilenetv2_preds_val.csv")
temp_json = Path("./artifacts/mobilenetv2_temp_scaling.json")

# √âvaluer avec la temp√©rature de r√©f√©rence
report1 = evaluate_csv_metrics(val_csv, temp_json=temp_json, plot=False)

# Cr√©er une temp√©rature diff√©rente
T_original = load_temperature(temp_json)
T_modified = T_original * 1.5  # Augmenter de 50%
temp_modified_json = Path("/tmp/temp_modified.json")
with open(temp_modified_json, "w") as f:
    json.dump({"temperature": T_modified}, f)

# √âvaluer avec la temp√©rature modifi√©e
report2 = evaluate_csv_metrics(val_csv, temp_json=temp_modified_json, plot=False)

print(f"ECE avec T={T_original:.3f} : {report1.ece_cal:.4f}")
print(f"ECE avec T={T_modified:.3f} : {report2.ece_cal:.4f}")
print("‚úÖ R√©utilisation du CSV avec diff√©rentes temp√©ratures fonctionne")
```

### Cellule : Test de performance
```python
# Mesurer les performances des fonctions principales
import time
import torch
from metrics.cnn_eval import compute_ece, fit_temperature

# Test compute_ece
sizes = [100, 1000, 10000]
for n in sizes:
    probs = torch.rand(n, 9)
    probs = probs / probs.sum(dim=1, keepdim=True)
    y_true = torch.randint(0, 9, (n,))
    
    start = time.time()
    ece = compute_ece(probs, y_true)
    elapsed = time.time() - start
    print(f"compute_ece({n} samples): {elapsed*1000:.2f}ms")

# Test fit_temperature
for n in [100, 1000]:
    logits = torch.randn(n, 9)
    y_true = torch.randint(0, 9, (n,))
    
    start = time.time()
    scaler = fit_temperature(logits, y_true, max_iter=100)
    elapsed = time.time() - start
    print(f"fit_temperature({n} samples, 100 iter): {elapsed:.2f}s")
```

## üéØ Priorit√©s

**Haute priorit√©** :
1. Tests de robustesse (CSV invalides, NaN/Inf)
2. Tests de coh√©rence (m√©triques coh√©rentes)
3. Tests de r√©utilisation (fichiers r√©utilisables)

**Moyenne priorit√©** :
4. Tests unitaires des fonctions individuelles
5. Tests d'int√©gration du pipeline complet

**Basse priorit√©** :
6. Tests de performance
7. Tests de visualisation

