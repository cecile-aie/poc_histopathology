
-----

# üß† PixCell Adapt√© : Fine-tuning Hybride (Adapter + LoRA)

Ce module impl√©mente une strat√©gie de **Fine-Tuning Efficace (PEFT)** pour adapter le mod√®le g√©n√©ratif PixCell au domaine sp√©cifique de l'histopathologie colorectale (dataset NCT-CRC-HE).

Contrairement √† l'approche "na√Øve" (Zero-Shot), cette architecture permet d'entra√Æner des composants l√©gers pour corriger le **Domain Shift** (d√©rive de couleur et de texture) tout en conservant la puissance s√©mantique des Foundation Models pr√©-entra√Æn√©s.

## üìê Architecture Technique

Le pipeline repose sur une architecture hybride combinant des mod√®les gel√©s (*Frozen*) et des modules entra√Ænables (*Trainable*).

```mermaid
graph TD
    %% Entr√©e
    Input["Image R√©elle H&E"] -->|"Resize 224x224"| A["UNI2-h Backbone<br/>(Frozen)"]
    
    %% Style Frozen (Bleu clair)
    style A fill:#e1f5fe,stroke:#0277bd,stroke-width:2px

    %% Adapter
    A -->|"Embedding 1536d"| B["Adapter MLP<br/>(Trainable)"]
    
    %% Style Trainable (Orange clair)
    style B fill:#ffccbc,stroke:#ff5722,stroke-width:2px
    
    %% Connexion vers l'int√©rieur du U-Net
    B -->|"Conditioning Vector"| D
    
    %% Bloc U-Net
    subgraph UNetBox ["PixCell U-Net Wrapper"]
        direction TB
        D["Attention Layers<br/>(Frozen)"]
        E["LoRA Layers<br/>(Trainable)"]
        
        %% Couleurs internes
        style D fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
        style E fill:#ffccbc,stroke:#ff5722,stroke-width:2px
        
        %% Interaction LoRA
        D <==>|"Injection Poids"| E
    end
    
    %% Sortie
    D -->|"Denoising"| Output["Image Synth√©tique"]
```

### 1\. Le "Pont" S√©mantique : L'Adapter

Le mod√®le UNI2-h produit des embeddings de dimension $1 \times 1536$. Le mod√®le PixCell, con√ßu √† l'origine pour du texte ou d'autres modalit√©s, ne peut ing√©rer ces vecteurs bruts.

  * **R√¥le :** L'Adapter est un r√©seau dense (MLP) l√©ger qui projette l'espace latent d'UNI2-h vers l'espace de conditionnement de PixCell.
  * **Pourquoi l'entra√Æner ?** Il apprend √† "traduire" les caract√©ristiques m√©dicales extraites par UNI2-h (forme des noyaux, densit√©) en instructions de g√©n√©ration compr√©hensibles par le U-Net.

### 2\. La Texture Fine : LoRA (Low-Rank Adaptation)

Pour adapter le style visuel (colorim√©trie H\&E, grain de la lame) sans r√©-entra√Æner les milliards de param√®tres du U-Net (co√ªteux et instable), nous injectons des couches **LoRA**.

  * **M√©canisme d'Insertion :** Les LoRA ciblent sp√©cifiquement les couches d'**Attention (Self-Attention & Cross-Attention)** du U-Net.
  * **Fonctionnement :** Au lieu de modifier la matrice de poids $W$ du mod√®le, LoRA ajoute une d√©viation apprise $\Delta W$ d√©compos√©e en deux matrices de rang faible $A$ et $B$ (telles que $\Delta W = A \times B$).
      * $W_{frozen}$ reste inchang√©.
      * Seules les petites matrices $A$ et $B$ sont mises √† jour par r√©tropropagation.
  * **Avantage :** Cela permet de modifier le comportement profond du mod√®le (comment il "attend" au conditionnement) avec \< 1% de param√®tres suppl√©mentaires.

## ‚öôÔ∏è Strat√©gie d'Entra√Ænement

Le succ√®s de l'adaptation repose sur un r√©glage pr√©cis des hyperparam√®tres, critique pour √©viter le "catastrophic forgetting" ou le sur-apprentissage.

### Configuration Critique

  * **Pr√©cision Mixte (FP16) :** Indispensable pour r√©duire l'empreinte m√©moire VRAM et acc√©l√©rer le calcul des gradients sur les LoRA.
  * **Optimiseur :** `AdamW` avec un *learning rate* sp√©cifique pour les LoRA (g√©n√©ralement autour de `1e-4`), souvent diff√©rent de celui de l'Adapter.
  * **Noise Scheduler :** Utilisation de `DDPMScheduler` pour l'entra√Ænement (stabilit√©) et bascule vers `DDIMScheduler` ou `EulerAncestral` pour l'inf√©rence (vitesse).

### Param√®tres LoRA

  * **Rank (r) :** Fix√© √† `4` ou `8`. Un rang faible force le mod√®le √† capturer l'essence du style sans m√©moriser les images d'entra√Ænement par c≈ìur.
  * **Alpha :** Facteur d'√©chelle (scaling) d√©terminant l'influence des poids LoRA par rapport aux poids gel√©s. Un alpha √©lev√© renforce l'adaptation au nouveau domaine.

## üöÄ Entra√Ænement et Inf√©rence

### Lancement de l'entra√Ænement

Le script g√®re automatiquement le chargement des poids, l'injection des LoRA via `peft` et la boucle d'entra√Ænement.

```python
# Extrait de la configuration
config = {
    "mixed_precision": "fp16",
    "gradient_accumulation_steps": 4,  # Pour simuler un gros batch size
    "learning_rate": 1e-4,
    "lora_rank": 8,
    "lora_target_modules": ["to_k", "to_q", "to_v", "to_out.0"] # Cible les couches d'attention
}

# Lancer le training
python train_adapter_lora.py --config config.yaml
```

### Inf√©rence

Pour g√©n√©rer des images, le pipeline charge le U-Net de base, puis "fusionne" ou active les poids LoRA et l'Adapter.

```python
# Chargement
pipeline = load_pixcell_base()
pipeline.load_lora_weights("path/to/lora_weights.safetensors")
adapter = load_adapter("path/to/adapter.pth")

# G√©n√©ration
embedding = adapter(uni2h_encoder(input_image))
image = pipeline(embedding, num_inference_steps=50).images[0]
```

## üìä Performance et Apports

Cette approche permet de :

1.  **R√©duire le Domain Shift :** Les images g√©n√©r√©es respectent la distribution colorim√©trique du dataset cible (NCT).
2.  **Am√©liorer la Fid√©lit√© Morphologique :** Gr√¢ce √† l'Adapter, le conditionnement par UNI2-h est mieux respect√© qu'avec une approche na√Øve.
3.  **Modularit√© :** Les poids LoRA (\~100 Mo) peuvent √™tre partag√©s facilement sans fournir le mod√®le complet (plusieurs Go).