
---

# ğŸ§  PixCell + UNI2-h : Adapter + LoRA dans le notebook `08_UNI2h_Adapter_PixCell_LoRA`

Ce notebook met en place une adaptation **lÃ©gÃ¨re mais complÃ¨te** du modÃ¨le gÃ©nÃ©ratif **PixCell-256** au domaine NCT-CRC-HE, en combinant :

* un **Adapter** entraÃ®nable entre lâ€™encodeur **UNI2-h** et le backbone PixCell,
* des **LoRA â€œallÃ©gÃ©esâ€** injectÃ©es manuellement dans les couches dâ€™attention du transformeur PixCell,
* une **boucle de diffusion OOM-friendly** (micro-batch, gradient accumulation, gradient checkpointing).

---

## ğŸ“ Architecture effective dans ce notebook

Le pipeline complet peut se rÃ©sumer ainsi :

```mermaid
graph TD
    %% EntrÃ©e
    Input["Tuile rÃ©elle H&E (NCT-CRC-HE)"] -->|"224Ã—224"| UNI["UNI2-h<br/>(gelÃ©)"]

    %% UNI
    style UNI fill:#e1f5fe,stroke:#0277bd,stroke-width:2px

    UNI -->|"Embedding 1Ã—1536"| ADAPT["Adapter MLP<br/>(entraÃ®nable)"]
    style ADAPT fill:#ffccbc,stroke:#ff5722,stroke-width:2px

    ADAPT -->|"Context UNI (tokens)"| CTX["ctx UNI2-h<br/>(fp16/fp32)"]

    %% Bloc PixCell
    subgraph PIXCELL ["PixCell-256 (Diffusers)<br/>VAE SD3 + Transformer"]
        direction TB
        CTX --> ATT["Blocs d'attention<br/>(gelÃ©s)"]
        style ATT fill:#e1f5fe,stroke:#0277bd,stroke-width:2px

        ATT <==>|"Î”W = AÂ·B (LoRA)"| LORA["LoRA Rank-r<br/>(entraÃ®nable)"]
        style LORA fill:#ffccbc,stroke:#ff5722,stroke-width:2px

        ATT --> LAT["Latents dÃ©bruitÃ©s<br/>(zÌ‚_t-1)"]
    end

    LAT -->|"decode VAE SD3"| OUT["Tuile synthÃ©tique 256Ã—256"]
```

### Points clÃ©s :

* **UNI2-h** est entiÃ¨rement gelÃ© : il sert de *backbone sÃ©mantique* pour extraire un embedding 1Ã—1536 par image.
* Lâ€™**Adapter MLP** projette cet embedding UNI dans lâ€™espace de conditionnement attendu par PixCell (dimension de contexte du transformer).
* Les **LoRA** sont appliquÃ©es **dans les couches dâ€™attention** du transformer PixCell (self/cross-attention, sous-ensemble â€œallÃ©gÃ©â€ de modules).
* Le **VAE SD3** (`stabilityai/stable-diffusion-3.5-large`, sous-dossier `vae`) est utilisÃ© comme encodeur/dÃ©codeur de latents.

---

## ğŸ§© ParticularitÃ©s techniques de ce notebook

### 1. CompatibilitÃ© `diffusers` / `peft` / `transformers`

Les versions rÃ©centes de `diffusers` et `peft` attendent un module :

```python
transformers.modeling_layers.GradientCheckpointingLayer
```

qui nâ€™existe plus dans `transformers>=4.45`.
Pour empÃªcher `peft` de casser lâ€™import **alors quâ€™on ne lâ€™utilise pas rÃ©ellement**, le notebook :

* crÃ©e un **module factice** `transformers.modeling_layers` avec un stub `GradientCheckpointingLayer`,
* force la dÃ©sactivation du backend PEFT via des flags globaux (`USE_PEFT_BACKEND = False` cÃ´tÃ© diffusers),
* Ã©vite tout chargement automatique de LoRA via `peft` :
  ğŸ‘‰ **les LoRA sont gÃ©rÃ©es manuellement dans ce notebook, sans `PeftModel`.**

Cela permet :

* de charger `PixCell-256` comme **pipeline diffusers classique**,
* dâ€™attacher nos LoRA custom sans dÃ©pendre des conventions `peft`.

### 2. Chargement du pipeline PixCell

Le pipeline est construit explicitement :

* VAE SD3 externe (recommandÃ© par les auteurs de PixCell),
* pipeline PixCell avec code distant (`trust_remote_code=True`).

En pratique :

```python
sd3_vae = AutoencoderKL.from_pretrained(
    "stabilityai/stable-diffusion-3.5-large",
    subfolder="vae",
    torch_dtype=DTYPE,
)

pipe = DiffusionPipeline.from_pretrained(
    "StonyBrook-CVLab/PixCell-256",
    vae=sd3_vae,
    custom_pipeline="StonyBrook-CVLab/PixCell-pipeline",
    trust_remote_code=True,
    torch_dtype=DTYPE,
).to(device)

_pipe = pipe  # alias utilisÃ© partout dans le notebook
```

Le **backbone de diffusion** (transformer PixCell) est ensuite rÃ©fÃ©rencÃ© via un alias (`BACKBONE`) pour la boucle dâ€™entraÃ®nement.

### 3. Adapter UNI2-h â†’ contexte PixCell

Lâ€™Adapter est un petit MLP :

* entrÃ©e : tensor `[B, 1, 1536]` issu de UNI2-h,
* sortie : `[B, T, C_ctx]` (dimension de contexte attendue par PixCell),
* entraÃ®nÃ© en **FP32**, puis cast en `MODEL_DTYPE` (FP16) pour rester compatible avec le backbone.

Il est appliquÃ© **Ã  la fois** :

* sur lâ€™**embedding positif** (image rÃ©elle),
* sur lâ€™**embedding â€œunconditionalâ€** rÃ©cupÃ©rÃ© via `_pipe.get_unconditional_embedding(B)` pour la guidance.

---

## âš™ï¸ Boucle dâ€™entraÃ®nement : diffusion + LoRA + Adapter

Lâ€™entraÃ®nement suit le schÃ©ma classique des modÃ¨les de diffusion, adaptÃ© Ã  PixCell :

1. **Dataset & loader**

   * Dataset : **NCT-CRC-HE-100K** (`data/NCT-CRC-HE-100K`), 9 classes histo (`ADI, BACK, DEB, LYM, MUC, MUS, NORM, STR, TUM`).
   * Chaque sample renvoie une image RGB 256Ã—256, re-scalÃ©e en **[-1, 1]** pour le VAE.
   * Les labels de classe sont disponibles pour lâ€™Ã©quilibrage (sampler), mais **aucune perte de classification nâ€™est encore utilisÃ©e** dans ce notebook :
     ğŸ‘‰ les LoRA sont conditionnÃ©es **uniquement via UNI2-h**, pas via une tÃªte de classe.

2. **Passage dans le VAE**

   * encodage dans lâ€™espace latent,
   * redimensionnement et scaling via `VAE.config.scaling_factor`.

3. **Bruitage / scheduler**

   * choix alÃ©atoire dâ€™un timestep `t`,
   * ajout de bruit gaussien `Îµ` sur les latents,
   * utilisation du scheduler Diffusers configurÃ© pour **apprentissage en mode â€œÎµ-predictionâ€** (ou `v_prediction` selon config).

4. **Conditionnement UNI2-h**

   * conversion batch `x âˆˆ [-1,1]` â†’ `[0,1]` â†’ liste de `PIL.Image`,
   * extraction des embeddings UNI2-h (`uni_embeds_for_pixcell`),
   * passage dans lâ€™Adapter pour obtenir les contextes `pos_ctx` et `neg_ctx`,
   * construction de `added_cond_kwargs` (rÃ©solution, aspect ratio, etc.) selon les conventions PixCell.

5. **PrÃ©diction de bruit par le backbone**

   * appel au transformer PixCell (`BACKBONE`) avec :

     * `noisy_latents`,
     * `timesteps`,
     * `encoder_hidden_states=pos_ctx`,
     * `added_cond_kwargs=...`,
   * rÃ©cupÃ©ration de `model_pred` (bruit prÃ©dit, Ã©ventuellement concat `[Îµ | log_var]` selon la config).

6. **Loss & optimisation**

   * **Loss principale** : MSE entre `model_pred` (ou sa composante Îµ) et la cible `target` dÃ©finie par le scheduler,
   * normalisation par `ACCUM_STEPS`,
   * **micro-batching** (`MICRO_BATCH = 1`) + **gradient accumulation** (`ACCUM_STEPS` > 1),
   * **gradient checkpointing** + **tiling VAE** pour tenir dans la VRAM,
   * optimisation avec `AdamW` sur :

     * paramÃ¨tres LoRA (attention layers ciblÃ©es),
     * paramÃ¨tres de lâ€™Adapter.

> ğŸ“Œ Ã€ ce stade, **aucune supervision explicite par classe** nâ€™est utilisÃ©e :
> la sÃ©paration inter-classes repose uniquement sur ce que UNI2-h encode dÃ©jÃ  dans ses embeddings.

---

## ğŸ§ª Mini-gÃ©nÃ©ration de contrÃ´le

Le notebook contient une â€œmini-gÃ©nÃ©rationâ€ :

* Ã©chantillonne quelques tuiles rÃ©elles (1 par classe),
* extrait leurs embeddings UNI2-h,
* passe par lâ€™Adapter,
* gÃ©nÃ¨re des tuiles synthÃ©tiques avec `_pipe` (PixCell + LoRA + Adapter),
* affiche un petit panel **rÃ©fÃ©rence / synthÃ©tique** par classe.

Cette Ã©tape sert Ã  vÃ©rifier rapidement :

* que le pipeline est cohÃ©rent (pas dâ€™erreur de shapes / dtype),
* que les LoRA sont bien prises en compte (visible au niveau du style),
* lâ€™impact visuel de lâ€™adaptation (mÃªme si certaines classes peuvent Ãªtre visuellement dominantes faute de contrainte de classe explicite).

---

## ğŸ“Š GÃ©nÃ©ration dâ€™un dataset synthÃ©tique pour FID / LPIPS

En fin de notebook, une cellule gÃ©nÃ¨re un **dataset synthÃ©tique organisÃ© par classe**, destinÃ© aux mÃ©triques dâ€™Ã©valuation (FID, LPIPS, downstream, etc.) :

* **Chemin de sortie :**
  `outputs/08_uni2h_adapter_lora/synthetic_dataset/`

* **Organisation :**
  un sous-dossier par code de classe NCT :

  ```text
  synthetic_dataset/
    ADI/
      gen_0000.png
      ...
    BACK/
    DEB/
    LYM/
    MUC/
    MUS/
    NORM/
    STR/
    TUM/
  ```

* **ParamÃ¨tres :**

  * `NUM_IMAGES_PER_CLASS` (par dÃ©faut 50),
  * `GUIDANCE_SCALE`,
  * `NUM_INFERENCE_STEPS`,
  * seed global `SEED_GEN` pour la reproductibilitÃ©.

* **StratÃ©gie :**

  * si le dossier rÃ©el `data/NCT-CRC-HE-100K/<CLASS_CODE>` existe et contient des images :

    * on Ã©chantillonne jusquâ€™Ã  `NUM_IMAGES_PER_CLASS` tuiles rÃ©elles,
    * pour chacune, on extrait UNI2-h, on passe par lâ€™Adapter, et on gÃ©nÃ¨re une image conditionnelle,
  * sinon :

    * on gÃ©nÃ¨re **sans rÃ©fÃ©rence** (unconditional) avec lâ€™embedding nÃ©gatif.

* **QualitÃ© dâ€™UX :**

  * une seule barre de progression par classe (via `tqdm`),
  * les barres internes Diffusers sont dÃ©sactivÃ©es pour ne pas polluer la sortie.

Ce dossier est ensuite consommÃ© par les notebooks de mÃ©triques (`fid_lpips_eval.py`, notebooks dÃ©diÃ©s aux FID/LPIPS/downstream).

---

## ğŸ”š Limitations actuelles & pistes dâ€™amÃ©lioration

Limitations assumÃ©es dans ce notebook :

* Pas de **tÃªte de classification** ni de **perte de classe** :

  * les LoRA et lâ€™Adapter nâ€™apprennent pas Ã  â€œsÃ©parerâ€ explicitement les 9 classes,
  * la variabilitÃ© inter-classes dÃ©pend uniquement de ce que lâ€™embedder UNI2-h encode dÃ©jÃ .
* LoRA â€œallÃ©gÃ©esâ€ :

  * seules certaines couches dâ€™attention sont Ã©quipÃ©es de LoRA,
  * le rang est faible pour rester lÃ©ger, ce qui limite la capacitÃ© Ã  modÃ©liser des variations fines par classe.

Pistes naturelles pour une version â€œv2â€ :

1. **Ajouter une petite tÃªte de classification sur le contexte UNI**
   â†’ Cross-entropy sur les labels NCT, combinÃ©e Ã  la loss diffusion.

2. **Introduire un conditioning explicite par classe**
   â†’ embedding de classe ajoutÃ© / concatÃ©nÃ© au contexte UNI avant dâ€™entrer dans le transformer PixCell.

3. **Explorer des LoRA plus riches ou ciblÃ©es par bloc**
   â†’ par exemple LoRA spÃ©cifiques aux blocs les plus sensibles Ã  la morphologie.

---

## ğŸ“ RÃ©sumÃ©

Ce notebook concrÃ©tise une **adaptation fine de PixCell Ã  lâ€™histopathologie colorectale** via :

* un **Adapter UNI2-h â†’ PixCell** entraÃ®nable,
* des **LoRA manuelles** sur les couches dâ€™attention,
* une boucle de diffusion robuste aux contraintes de VRAM,
* un pipeline de **gÃ©nÃ©ration synthÃ©tique par classe** prÃªt pour les mÃ©triques (FID, LPIPS, downstream).

Il sert de socle â€œpropreâ€ pour tester ensuite des variantes plus ambitieuses (conditionnement explicite par classe, pertes supplÃ©mentaires, expÃ©rimentations sur la variabilitÃ© inter-classes).
