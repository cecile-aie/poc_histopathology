
---

# ğŸ§  Baseline Classification CNN â€“ README Technique

Ce document dÃ©taille la baseline de **classification histopathologique** utilisÃ©e dans le projet : pipeline complet, stratÃ©gie dâ€™entraÃ®nement, prÃ©processing, mÃ©triques dâ€™Ã©valuation et calibration probabiliste.

---

# 1. ğŸ¯ Objectifs de la baseline

* Fournir une **rÃ©fÃ©rence robuste** pour juger lâ€™apport des images synthÃ©tiques (cGAN, PixCell).
* Ã‰valuer les performances sur les **9 classes histopathologiques** dÃ©finies dans `class_mappings.py`.
* Exporter des mÃ©triques dÃ©taillÃ©es pour les analyses downstream.

---

# 2. ğŸ“ Jeu de donnÃ©es et splits

* **Train** : NCT-CRC-HE-100K
* **Validation / Test** : CRC-VAL-HE-7K
* **Aucune fuite** : les images sÃ©lectionnÃ©es pour la gÃ©nÃ©ration synthÃ©tique sont explicitement exclues du test.
* PossibilitÃ© d'Ã©quilibrer le train via un sampler.

Les classes traitÃ©es :

```python
{
  "ADI":  "Tissu adipeux",
  "BACK": "ArriÃ¨re-plan",
  "DEB":  "DÃ©bris / nÃ©crose",
  "LYM":  "Lymphocytes",
  "MUC":  "Mucus",
  "MUS":  "Muscle lisse",
  "NORM": "Muqueuse normale",
  "STR":  "Stroma",
  "TUM":  "Ã‰pithÃ©lium tumoral"
}
```

---

# 3. ğŸ§¼ PrÃ©processing avancÃ© (HistoDataset)

## 3.1 Chargement dâ€™image

* Lecture PIL â†’ conversion RGB.
* Redimensionnement en **256Ã—256**.
* Normalisation **ImageNet** pour MobileNetV2.

## 3.2 Filtre qualitÃ©

ImplÃ©mentÃ© dans `QualityFilter` (features calculÃ©es via NumPy/Scipy) :

* **Laplace variance** (nettetÃ©)
* **Entropie**
* **Tenengrad** (gradient)
* **White ratio**, **Saturation ratio**
* **Blockiness spatial et DCT**
* **Tissue fraction**

RÃ¨gles spÃ©cifiques :

* `BACK` : toujours acceptÃ©.
* `ADI` : seuils permissifs.
* Autres classes : seuils stricts.

## 3.3 Normalisation de coloration (optionnelle)

* BasÃ©e sur **Vahadane** via `torch_staintools`.
* Initialisation stable + fallback automatique en cas dâ€™erreur.
* Compatible CPU/GPU.

---

# 4. ğŸ§  ModÃ¨le â€“ MobileNetV2

* Backbone prÃ©-entraÃ®nÃ© ImageNet.
* TÃªte remplacÃ©e par un **classifier 9 classes**.
* Activation finale : logits (pas de softmax dans le modÃ¨le).

Avantages :

* TrÃ¨s lÃ©ger, rapide en infÃ©rence.
* Bon compromis entre vitesse et expressivitÃ©.

---

# 5. âš™ï¸ StratÃ©gie dâ€™entraÃ®nement

## 5.1 Fine-tuning progressif

* **Phase 1** : backbone gelÃ© â†’ stabilisation des gradients.
* **Phase 2** : dÃ©gel progressif des derniers blocs pour adapter aux motifs H&E.

## 5.2 Optimisation

* Optimiseur : Adam ou SGD (selon notebook).
* **Scheduler** : dÃ©croissance du LR ou ReduceLROnPlateau.
* **Early stopping** sur la perte validation.

## 5.3 DataLoader

* GPU-friendly : `pin_memory=True`, `non_blocking=True`.
* Shuffle systÃ©matique.
* Retour `(image, label, path)` pour analyses downstream.

---

# 6. ğŸ“Š Pipeline dâ€™Ã©valuation (cnn_eval.py)

Le module `cnn_eval.py` offre un pipeline complet dâ€™Ã©valuation et de calibration.

### 6.1 Export CSV

Pour chaque image :

* chemin
* label rÃ©el
* prÃ©diction
* confiance brute
* **logits JSON**
* confiance calibrÃ©e (optionnel)

Exemple :

```python
df = export_predictions(
    model, dataloader, device,
    out_csv="preds_val.csv",
    split="val",
    class_to_idx=train_ds.class_to_idx
)
```

### 6.2 MÃ©triques calculÃ©es

* **Accuracy (micro)** : performance globale.
* **F1 macro** : indispensable pour dataset dÃ©sÃ©quilibrÃ©.
* **Matrice de confusion**.
* **Reliability diagram** (calibration).
* **ECE** : Expected Calibration Error.
* **Brier score** : qualitÃ© probabiliste.

---

# 7. ğŸ›ï¸ Calibration â€“ Temperature Scaling (T-scaling)

## 7.1 Pourquoi calibrer ?

Les CNN sont souvent **trop confiants**.
La calibration ajuste uniquement les probabilitÃ©s, pas les prÃ©dictions.

## 7.2 Principe

On apprend un scalaire **T** tel que :

[
p_i = \text{softmax}(z_i / T)
]

* Si **T > 1** â†’ modÃ¨le *moins confiant*.
* Si **T < 1** â†’ modÃ¨le *plus confiant*.
* Ajustement global, simple, trÃ¨s efficace.

## 7.3 Comment T est appris ?

* On rÃ©cupÃ¨re les **logits val** via le CSV.
* On minimise la **CrossEntropy** avec LBFGS.
* On sauvegarde T dans un `.json`.

```python
T = fit_temperature_from_csv("preds_val.csv", "temperature.json")
```

## 7.4 Application Ã  lâ€™infÃ©rence test

Les logits sont divisÃ©s par T avant le softmax.

---

# 8. ğŸ§® MÃ©trologies clÃ©s (ECE, Brier, etc.)

## 8.1 ECE â€“ Expected Calibration Error

* On discretise les prÃ©dictions par bins de confiance (10 ou 15 bins).
* Pour chaque bin :

  * **confiance moyenne**
  * **accuracy moyenne**
* ECE = somme pondÃ©rÃ©e des Ã©carts |acc â€“ conf|.

InterprÃ©tation :

* **0%** = calibration parfaite.
* **>5%** = sur-confiance typique des CNN.

## 8.2 Brier score

[
\text{Brier}=\frac{1}{N}\sum(p_{\text{pred}} - y_{\text{true}})^2
]

* Plus bas = mieux.
* Mesure la qualitÃ© probabiliste brute.

## 8.3 Accuracy & F1 macro

* Accuracy = performance globale.
* F1 macro = Ã©quilibre inter-classes, critique en histopathologie (certaines classes rares).

---

# 9. ğŸ§ª RÃ©sultats typiques

(Ã€ adapter selon le run.)

* Bonnes performances sur classes frÃ©quentes : **NORM**, **STR**, **TUM**.
* Scores plus faibles sur classes rares : **LYM**, **DEB**, **MUS**.
* **Calibration amÃ©liore nettement lâ€™ECE** sans modifier les prÃ©dictions.
* Baseline robuste pour comparaison **real vs synth (cGAN, PixCell)**.

---

# 10. ğŸ“¦ Artefacts gÃ©nÃ©rÃ©s

* `preds_val.csv`, `preds_test.csv`
* `temperature.json`
* `confusion_matrix.png`
* `reliability_diagram.png`
* `metrics.json`

---

# 11. ğŸš€ RÃ´le dans le projet

Cette baseline sert de **rÃ©fÃ©rence principale** pour :

* Ã©valuer lâ€™utilitÃ© des images synthÃ©tiques,
* mesurer la cohÃ©rence du classifieur sur donnÃ©es GAN/diffusion,
* produire les analyses downstream (PCR, Consistency, KL, â€¦).

---
